{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwBs3MiJn2pU",
        "outputId": "4003494c-6c67-4bce-bbe0-e5fdf4585e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.0/618.0 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Colab cell (Code)\n",
        "!pip install -q kaggle==1.5.12\n",
        "!pip install -q ultralytics==8.0.185  # YOLOv8 (specific version may vary)\n",
        "!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q pycocotools pandas pillow tqdm matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"nikhilsanga1206\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"16c0c673e500366baf8a2262a8494565\"\n",
        "\n",
        "print(\"Kaggle API Ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwYVxtxTn_n3",
        "outputId": "4153f826-2a21-4ee2-82f9-f861b8b3ec1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle API Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab code cell\n",
        "# Change dataset_slug to the actual Kaggle dataset slug you want to download.\n",
        "# For your NIH chest x-ray example:\n",
        "DATASET_SLUG=\"khanfashee/nih-chest-x-ray-14-224x224-resized\"\n",
        "!mkdir -p /content/datasets\n",
        "!kaggle datasets download -d $DATASET_SLUG -p /content/datasets --unzip\n",
        "!ls -lh /content/datasets | sed -n '1,120p'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctCxRhqeoLy0",
        "outputId": "32670dcc-d153-4cb4-bc85-805651d35bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nih-chest-x-ray-14-224x224-resized.zip to /content/datasets\n",
            "100% 2.30G/2.30G [01:48<00:00, 24.5MB/s]\n",
            "100% 2.30G/2.30G [01:48<00:00, 22.8MB/s]\n",
            "total 38M\n",
            "-rw-r--r-- 1 root root  91K Nov 14 13:56 BBox_List_2017_Official_NIH.csv\n",
            "-rw-r--r-- 1 root root 8.3M Nov 14 13:56 Data_Entry_2017.csv\n",
            "drwxr-xr-x 3 root root 4.0K Nov 14 13:56 images-224\n",
            "-rw-r--r-- 1 root root  28M Nov 14 13:56 pretrained_model.h5\n",
            "-rw-r--r-- 1 root root 425K Nov 14 13:56 test_list_NIH.txt\n",
            "-rw-r--r-- 1 root root 1.5M Nov 14 13:56 train_val_list_NIH.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab code cell\n",
        "import os, glob\n",
        "for f in glob.glob('/content/datasets/*'):\n",
        "    print(f)\n",
        "# Also list top of dataset folder (images + CSVs)\n",
        "!ls -lah /content/datasets | sed -n '1,200p'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjW9umNmoAZq",
        "outputId": "7852e008-4810-41f0-adfe-3a7254e4f666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets/Data_Entry_2017.csv\n",
            "/content/datasets/images-224\n",
            "/content/datasets/train_val_list_NIH.txt\n",
            "/content/datasets/BBox_List_2017_Official_NIH.csv\n",
            "/content/datasets/test_list_NIH.txt\n",
            "/content/datasets/pretrained_model.h5\n",
            "total 38M\n",
            "drwxr-xr-x 3 root root 4.0K Nov 14 13:56 .\n",
            "drwxr-xr-x 1 root root 4.0K Nov 14 13:54 ..\n",
            "-rw-r--r-- 1 root root  91K Nov 14 13:56 BBox_List_2017_Official_NIH.csv\n",
            "-rw-r--r-- 1 root root 8.3M Nov 14 13:56 Data_Entry_2017.csv\n",
            "drwxr-xr-x 3 root root 4.0K Nov 14 13:56 images-224\n",
            "-rw-r--r-- 1 root root  28M Nov 14 13:56 pretrained_model.h5\n",
            "-rw-r--r-- 1 root root 425K Nov 14 13:56 test_list_NIH.txt\n",
            "-rw-r--r-- 1 root root 1.5M Nov 14 13:56 train_val_list_NIH.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab code cell\n",
        "import pandas as pd\n",
        "META = \"/content/datasets/Data_Entry_2017.csv\"\n",
        "BBOX = \"/content/datasets/BBox_List_2017_Official_NIH.csv\"\n",
        "print(\"Files exist?\", pd.io.common.file_exists(META), pd.io.common.file_exists(BBOX))\n",
        "\n",
        "meta_df = pd.read_csv(META)\n",
        "bbox_df = pd.read_csv(BBOX)\n",
        "\n",
        "# Clean column names\n",
        "meta_df.columns = meta_df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "bbox_df.columns = bbox_df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "\n",
        "print(\"meta cols:\", meta_df.columns.tolist()[:10])\n",
        "print(\"bbox cols:\", bbox_df.columns.tolist()[:20])\n",
        "print(\"Sample bbox rows:\")\n",
        "bbox_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "JM4lt7lJp0pH",
        "outputId": "0a671231-a551-4e71-efc7-2bb206b71ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files exist? True True\n",
            "meta cols: ['image_index', 'finding_labels', 'follow-up_#', 'patient_id', 'patient_age', 'patient_gender', 'view_position', 'originalimage[width', 'height]', 'originalimagepixelspacing[x']\n",
            "bbox cols: ['image_index', 'finding_label', 'bbox_[x', 'y', 'w', 'h]', 'unnamed:_6', 'unnamed:_7', 'unnamed:_8']\n",
            "Sample bbox rows:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        image_index finding_label     bbox_[x           y           w  \\\n",
              "0  00013118_008.png   Atelectasis  225.084746  547.019217   86.779661   \n",
              "1  00014716_007.png   Atelectasis  686.101695  131.543498  185.491525   \n",
              "2  00029817_009.png   Atelectasis  221.830508  317.053115  155.118644   \n",
              "3  00014687_001.png   Atelectasis  726.237288  494.951420  141.016949   \n",
              "4  00017877_001.png   Atelectasis  660.067797  569.780787  200.677966   \n",
              "\n",
              "           h]  unnamed:_6  unnamed:_7  unnamed:_8  \n",
              "0   79.186441         NaN         NaN         NaN  \n",
              "1  313.491525         NaN         NaN         NaN  \n",
              "2  216.949153         NaN         NaN         NaN  \n",
              "3   55.322034         NaN         NaN         NaN  \n",
              "4   78.101695         NaN         NaN         NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e66184f5-d0d5-46f0-9064-4925f397ebfd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_index</th>\n",
              "      <th>finding_label</th>\n",
              "      <th>bbox_[x</th>\n",
              "      <th>y</th>\n",
              "      <th>w</th>\n",
              "      <th>h]</th>\n",
              "      <th>unnamed:_6</th>\n",
              "      <th>unnamed:_7</th>\n",
              "      <th>unnamed:_8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00013118_008.png</td>\n",
              "      <td>Atelectasis</td>\n",
              "      <td>225.084746</td>\n",
              "      <td>547.019217</td>\n",
              "      <td>86.779661</td>\n",
              "      <td>79.186441</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00014716_007.png</td>\n",
              "      <td>Atelectasis</td>\n",
              "      <td>686.101695</td>\n",
              "      <td>131.543498</td>\n",
              "      <td>185.491525</td>\n",
              "      <td>313.491525</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00029817_009.png</td>\n",
              "      <td>Atelectasis</td>\n",
              "      <td>221.830508</td>\n",
              "      <td>317.053115</td>\n",
              "      <td>155.118644</td>\n",
              "      <td>216.949153</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00014687_001.png</td>\n",
              "      <td>Atelectasis</td>\n",
              "      <td>726.237288</td>\n",
              "      <td>494.951420</td>\n",
              "      <td>141.016949</td>\n",
              "      <td>55.322034</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017877_001.png</td>\n",
              "      <td>Atelectasis</td>\n",
              "      <td>660.067797</td>\n",
              "      <td>569.780787</td>\n",
              "      <td>200.677966</td>\n",
              "      <td>78.101695</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e66184f5-d0d5-46f0-9064-4925f397ebfd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e66184f5-d0d5-46f0-9064-4925f397ebfd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e66184f5-d0d5-46f0-9064-4925f397ebfd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7a69d33f-d068-4c65-9088-0ac691c78a80\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a69d33f-d068-4c65-9088-0ac691c78a80')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7a69d33f-d068-4c65-9088-0ac691c78a80 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "bbox_df",
              "summary": "{\n  \"name\": \"bbox_df\",\n  \"rows\": 984,\n  \"fields\": [\n    {\n      \"column\": \"image_index\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 880,\n        \"samples\": [\n          \"00018366_029.png\",\n          \"00009745_000.png\",\n          \"00017972_026.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"finding_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Cardiomegaly\",\n          \"Nodule\",\n          \"Atelectasis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox_[x\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 222.700868122465,\n        \"min\": 5.41798941798942,\n        \"max\": 905.887830687831,\n        \"num_unique_values\": 705,\n        \"samples\": [\n          196.266666666667,\n          538.548148148148,\n          232.973544973545\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 166.30999463959026,\n        \"min\": 12.8379340277778,\n        \"max\": 876.980783236229,\n        \"num_unique_values\": 700,\n        \"samples\": [\n          545.934471001059,\n          247.22015625,\n          535.077934027778\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 167.62961953840582,\n        \"min\": 27.3066666666667,\n        \"max\": 901.12,\n        \"num_unique_values\": 608,\n        \"samples\": [\n          179.877248677249,\n          227.796610169492,\n          430.64406779661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"h]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 159.44363511494944,\n        \"min\": 21.6177777777778,\n        \"max\": 873.379894179894,\n        \"num_unique_values\": 647,\n        \"samples\": [\n          98.6074074074074,\n          359.754497354497,\n          215.635978835979\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unnamed:_6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unnamed:_7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unnamed:_8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "from pathlib import Path\n",
        "\n",
        "IMG_DIR = Path(\"/content/datasets/images-224/images-224\")\n",
        "COCO_OUT = \"/content/coco_chestxray.json\"\n",
        "\n",
        "bbox_df_clean = bbox_df.copy()\n",
        "\n",
        "# COLUMN NAMES (after cleaning)\n",
        "img_col = \"image_index\"\n",
        "label_col = \"finding_label\"\n",
        "\n",
        "# Correct NIH Bounding Box Columns\n",
        "x1_col = \"bbox_x1\"\n",
        "y1_col = \"bbox_y1\"\n",
        "x2_col = \"bbox_x2\"\n",
        "y2_col = \"bbox_y2\"\n",
        "\n",
        "# Build categories\n",
        "unique_labels = sorted(bbox_df_clean[label_col].unique().tolist())\n",
        "categories = [{\"id\": i+1, \"name\": name} for i, name in enumerate(unique_labels)]\n",
        "label2id = {name: i+1 for i, name in enumerate(unique_labels)}\n",
        "\n",
        "images = []\n",
        "annotations = []\n",
        "img_id_map = {}\n",
        "next_img_id = 1\n",
        "next_ann_id = 1\n",
        "\n",
        "for _, row in bbox_df_clean.iterrows():\n",
        "    fname = str(row[img_col])\n",
        "    label = row[label_col]\n",
        "\n",
        "    # Extract bounding box\n",
        "    try:\n",
        "        x1 = float(row[x1_col])\n",
        "        y1 = float(row[y1_col])\n",
        "        x2 = float(row[x2_col])\n",
        "        y2 = float(row[y2_col])\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    w = x2 - x1\n",
        "    h = y2 - y1\n",
        "\n",
        "    # Add image entry if not already\n",
        "    if fname not in img_id_map:\n",
        "        img_id_map[fname] = next_img_id\n",
        "        images.append({\n",
        "            \"id\": next_img_id,\n",
        "            \"file_name\": fname,\n",
        "            \"width\": 224,\n",
        "            \"height\": 224\n",
        "        })\n",
        "        next_img_id += 1\n",
        "\n",
        "    # Add annotation\n",
        "    annotations.append({\n",
        "        \"id\": next_ann_id,\n",
        "        \"image_id\": img_id_map[fname],\n",
        "        \"category_id\": label2id[label],\n",
        "        \"bbox\": [x1, y1, w, h],\n",
        "        \"area\": float(w * h),\n",
        "        \"iscrowd\": 0\n",
        "    })\n",
        "    next_ann_id += 1\n",
        "\n",
        "# Final COCO dictionary\n",
        "coco = {\n",
        "    \"images\": images,\n",
        "    \"annotations\": annotations,\n",
        "    \"categories\": categories\n",
        "}\n",
        "\n",
        "# Save JSON\n",
        "with open(COCO_OUT, \"w\") as f:\n",
        "    json.dump(coco, f, indent=4)\n",
        "\n",
        "print(\"Wrote COCO:\", COCO_OUT)\n",
        "print(\"Images:\", len(images))\n",
        "print(\"Annotations:\", len(annotations))\n",
        "print(\"Categories:\", len(categories))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jrd_sZup2XG",
        "outputId": "0e1cb84a-8939-4402-fd09-84b209ffd6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote COCO: /content/coco_chestxray.json\n",
            "Images: 0\n",
            "Annotations: 0\n",
            "Categories: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bbox_df = pd.read_csv(\"/content/datasets/BBox_List_2017_Official_NIH.csv\")\n",
        "bbox_df.columns = bbox_df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "bbox_df.columns.tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEEv6Z_lqIDC",
        "outputId": "cc1f4b99-a729-4894-fbd0-52a1fbd5b175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['image_index',\n",
              " 'finding_label',\n",
              " 'bbox_[x',\n",
              " 'y',\n",
              " 'w',\n",
              " 'h]',\n",
              " 'unnamed:_6',\n",
              " 'unnamed:_7',\n",
              " 'unnamed:_8']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "from pathlib import Path\n",
        "\n",
        "IMG_DIR = Path(\"/content/datasets/images-224/images-224\")\n",
        "COCO_OUT = \"/content/coco_chestxray.json\"\n",
        "\n",
        "bbox_df_clean = bbox_df.copy()\n",
        "\n",
        "# Correct columns (confirmed)\n",
        "img_col = \"image_index\"\n",
        "label_col = \"finding_label\"\n",
        "x_col = \"bbox_[x\"\n",
        "y_col = \"y\"\n",
        "w_col = \"w\"\n",
        "h_col = \"h]\"\n",
        "\n",
        "# Build categories dictionary\n",
        "unique_labels = sorted(bbox_df_clean[label_col].unique().tolist())\n",
        "categories = [{\"id\": i+1, \"name\": name} for i, name in enumerate(unique_labels)]\n",
        "label2id = {name: i+1 for i, name in enumerate(unique_labels)}\n",
        "\n",
        "images = []\n",
        "annotations = []\n",
        "img_id_map = {}\n",
        "next_img_id = 1\n",
        "next_ann_id = 1\n",
        "\n",
        "for _, row in bbox_df_clean.iterrows():\n",
        "\n",
        "    fname = str(row[img_col])\n",
        "    label = row[label_col]\n",
        "\n",
        "    # Get bbox values\n",
        "    try:\n",
        "        x = float(row[x_col])\n",
        "        y = float(row[y_col])\n",
        "        w = float(row[w_col])\n",
        "        h = float(row[h_col])\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    # Add image entry once\n",
        "    if fname not in img_id_map:\n",
        "        img_id_map[fname] = next_img_id\n",
        "        images.append({\n",
        "            \"id\": next_img_id,\n",
        "            \"file_name\": fname,\n",
        "            \"width\": 224,\n",
        "            \"height\": 224\n",
        "        })\n",
        "        next_img_id += 1\n",
        "\n",
        "    # Add annotation\n",
        "    annotations.append({\n",
        "        \"id\": next_ann_id,\n",
        "        \"image_id\": img_id_map[fname],\n",
        "        \"category_id\": label2id[label],\n",
        "        \"bbox\": [x, y, w, h],\n",
        "        \"area\": w * h,\n",
        "        \"iscrowd\": 0\n",
        "    })\n",
        "    next_ann_id += 1\n",
        "\n",
        "# Save COCO dict\n",
        "coco = {\n",
        "    \"images\": images,\n",
        "    \"annotations\": annotations,\n",
        "    \"categories\": categories\n",
        "}\n",
        "\n",
        "with open(COCO_OUT, \"w\") as f:\n",
        "    json.dump(coco, f, indent=4)\n",
        "\n",
        "print(\"Wrote COCO:\", COCO_OUT)\n",
        "print(\"Images:\", len(images))\n",
        "print(\"Annotations:\", len(annotations))\n",
        "print(\"Categories:\", len(categories))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMxCCxU7qlEz",
        "outputId": "88b27b27-0c33-418b-d62b-8145ee94d38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote COCO: /content/coco_chestxray.json\n",
            "Images: 880\n",
            "Annotations: 984\n",
            "Categories: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "COCO_JSON = \"/content/coco_chestxray.json\"\n",
        "SRC_IMG_DIR = Path(\"/content/datasets/images-224/images-224\")\n",
        "\n",
        "OUT_ROOT = Path(\"/content/coco_dataset\")\n",
        "IMG_TRAIN = OUT_ROOT / \"images/train\"\n",
        "IMG_VAL = OUT_ROOT / \"images/val\"\n",
        "ANN_DIR = OUT_ROOT / \"annotations\"\n",
        "\n",
        "# Create folders\n",
        "IMG_TRAIN.mkdir(parents=True, exist_ok=True)\n",
        "IMG_VAL.mkdir(parents=True, exist_ok=True)\n",
        "ANN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load COCO JSON\n",
        "with open(COCO_JSON, \"r\") as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "images = coco[\"images\"]\n",
        "annotations = coco[\"annotations\"]\n",
        "categories = coco[\"categories\"]\n",
        "\n",
        "# Shuffle & split\n",
        "random.seed(42)\n",
        "image_ids = [img[\"id\"] for img in images]\n",
        "random.shuffle(image_ids)\n",
        "\n",
        "split = int(0.90 * len(image_ids))  # 90% train\n",
        "train_ids = set(image_ids[:split])\n",
        "val_ids = set(image_ids[split:])\n",
        "\n",
        "# Separate images and annotations\n",
        "train_images = [img for img in images if img[\"id\"] in train_ids]\n",
        "val_images   = [img for img in images if img[\"id\"] in val_ids]\n",
        "\n",
        "train_ann = [ann for ann in annotations if ann[\"image_id\"] in train_ids]\n",
        "val_ann   = [ann for ann in annotations if ann[\"image_id\"] in val_ids]\n",
        "\n",
        "# Copy images\n",
        "missing = 0\n",
        "for img in images:\n",
        "    fname = img[\"file_name\"]\n",
        "    src_path = SRC_IMG_DIR / fname\n",
        "\n",
        "    if not src_path.exists():\n",
        "        missing += 1\n",
        "        continue\n",
        "\n",
        "    if img[\"id\"] in train_ids:\n",
        "        shutil.copy(src_path, IMG_TRAIN / fname)\n",
        "    else:\n",
        "        shutil.copy(src_path, IMG_VAL / fname)\n",
        "\n",
        "# Save annotation files\n",
        "with open(ANN_DIR / \"instances_train.json\", \"w\") as f:\n",
        "    json.dump({\"images\": train_images, \"annotations\": train_ann, \"categories\": categories}, f, indent=4)\n",
        "\n",
        "with open(ANN_DIR / \"instances_val.json\", \"w\") as f:\n",
        "    json.dump({\"images\": val_images, \"annotations\": val_ann, \"categories\": categories}, f, indent=4)\n",
        "\n",
        "print(\"Train images:\", len(train_images))\n",
        "print(\"Val images:\", len(val_images))\n",
        "print(\"Train annotations:\", len(train_ann))\n",
        "print(\"Val annotations:\", len(val_ann))\n",
        "print(\"Missing images:\", missing)\n",
        "print(\"Dataset created at:\", OUT_ROOT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_fhGHL9qt_f",
        "outputId": "baad1910-4895-4512-b903-dee6b4a3ca67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 792\n",
            "Val images: 88\n",
            "Train annotations: 888\n",
            "Val annotations: 96\n",
            "Missing images: 0\n",
            "Dataset created at: /content/coco_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "root = Path(\"/content/coco_dataset\")\n",
        "data_yaml = root / \"data.yaml\"\n",
        "\n",
        "# category names\n",
        "with open(\"/content/coco_chestxray.json\", \"r\") as f:\n",
        "    coco_json = json.load(f)\n",
        "\n",
        "names = [cat[\"name\"] for cat in coco_json[\"categories\"]]\n",
        "nc = len(names)\n",
        "\n",
        "data_yaml.write_text(\n",
        "    f\"path: {root}\\n\"\n",
        "    f\"train: images/train\\n\"\n",
        "    f\"val: images/val\\n\"\n",
        "    f\"nc: {nc}\\n\"\n",
        "    f\"names: {names}\\n\"\n",
        ")\n",
        "\n",
        "print(\"Created:\", data_yaml)\n",
        "print(data_yaml.read_text())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztXF--Y4q2rM",
        "outputId": "b888f996-accc-4d7a-cb24-d5994e0666eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created: /content/coco_dataset/data.yaml\n",
            "path: /content/coco_dataset\n",
            "train: images/train\n",
            "val: images/val\n",
            "nc: 8\n",
            "names: ['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltrate', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.serialization\n",
        "import ultralytics.nn.tasks as tasks\n",
        "import inspect\n",
        "\n",
        "# Find ALL classes inside ultralytics.nn.tasks\n",
        "safe_list = []\n",
        "for name, obj in tasks.__dict__.items():\n",
        "    if inspect.isclass(obj):\n",
        "        safe_list.append(obj)\n",
        "\n",
        "torch.serialization.add_safe_globals(safe_list)\n",
        "\n",
        "print(\"Whitelisted YOLO model classes:\", [cls.__name__ for cls in safe_list])\n",
        "print(\"Safe globals added successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGO6CwU_q9Gj",
        "outputId": "d04135d7-bb62-4265-b94c-988ca029df79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitelisted YOLO model classes: ['Path', 'AIFI', 'C1', 'C2', 'C3', 'C3TR', 'SPP', 'SPPF', 'Bottleneck', 'BottleneckCSP', 'C2f', 'C3Ghost', 'C3x', 'Classify', 'Concat', 'Conv', 'Conv2', 'ConvTranspose', 'Detect', 'DWConv', 'DWConvTranspose2d', 'Focus', 'GhostBottleneck', 'GhostConv', 'HGBlock', 'HGStem', 'Pose', 'RepC3', 'RepConv', 'RTDETRDecoder', 'Segment', 'v8ClassificationLoss', 'v8DetectionLoss', 'v8PoseLoss', 'v8SegmentationLoss', 'BaseModel', 'DetectionModel', 'SegmentationModel', 'PoseModel', 'ClassificationModel', 'RTDETRDetectionModel', 'Ensemble']\n",
            "Safe globals added successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install ultralytics==8.1.34\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3qaXT2jzrPT-",
        "outputId": "614afa9d-4260-4864-c1dd-2da3e3dcdef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.4.0%2Bcu118-cp312-cp312-linux_x86_64.whl (857.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.7/857.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.0%2Bcu118-cp312-cp312-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.0%2Bcu118-cp312-cp312-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (75.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m948.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.20.5 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.20.5-py3-none-manylinux2014_x86_64.whl (142.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.9/142.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.20.5 nvidia-nvtx-cu11-11.8.86 torch-2.4.0+cu118 torchaudio-2.4.0+cu118 torchvision-0.19.0+cu118 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision",
                  "triton"
                ]
              },
              "id": "31e1a08d22ce4b7892cccc716ba85cef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics==8.1.34\n",
            "  Downloading ultralytics-8.1.34-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (2.4.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (0.19.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics==8.1.34)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.1.34) (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.1.34) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.1.34) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.1.34) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.1.34) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.1.34) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.1.34) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.1.34) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (11.8.86)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.1.34) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.1.34) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.1.34) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.8.0->ultralytics==8.1.34) (1.3.0)\n",
            "Downloading ultralytics-8.1.34-py3-none-any.whl (723 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m723.1/723.1 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "print(\"YOLO imported successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cFH2t6-rwfr",
        "outputId": "b29f9ffe-dea5-47f4-ef07-e0fd2d4ac77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect train model=yolov8n.pt data=/content/coco_dataset/data.yaml epochs=10 imgsz=640 batch=16 project=/content/yolo_train name=exp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqWJNo9Nr-ZJ",
        "outputId": "c878fe11-b5c4-4cdd-e578-975f93fa36d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "New https://pypi.org/project/ultralytics/8.3.228 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/coco_dataset/data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=/content/yolo_train, name=exp, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/content/yolo_train/exp\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 112MB/s]\n",
            "2025-11-14 14:08:41.198511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763129321.492720    5759 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763129321.574330    5759 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763129322.179505    5759 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763129322.179545    5759 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763129322.179550    5759 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763129322.179554    5759 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3012408 parameters, 3012392 gradients\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/yolo_train/exp', view at http://localhost:6006/\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/cfg/__init__.py\", line 445, in entrypoint\n",
            "    getattr(model, mode)(**overrides)  # default args from model\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\", line 337, in train\n",
            "    self.trainer.train()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 195, in train\n",
            "    self._do_train(world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 293, in _do_train\n",
            "    self._setup_train(world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 215, in _setup_train\n",
            "    self.run_callbacks('on_pretrain_routine_start')\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 160, in run_callbacks\n",
            "    callback(self)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/utils/callbacks/wb.py\", line 29, in on_pretrain_routine_start\n",
            "    wb.run or wb.init(project=trainer.args.project or 'YOLOv8', name=trainer.args.name, config=vars(trainer.args))\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_init.py\", line 1475, in init\n",
            "    init_settings.project = project\n",
            "    ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pydantic/main.py\", line 998, in __setattr__\n",
            "    setattr_handler(self, name, value)  # call here to not memo on possibly unknown fields\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pydantic/main.py\", line 114, in <lambda>\n",
            "    'validate_assignment': lambda model, name, val: model.__pydantic_validator__.validate_assignment(model, name, val),  # pyright: ignore[reportAssignmentType]\n",
            "                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_settings.py\", line 1297, in validate_project\n",
            "    raise UsageError(\n",
            "wandb.errors.errors.UsageError: Invalid project name '/content/yolo_train': cannot contain characters '/,\\\\,#,?,%,:', found '/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "print(\"W&B disabled successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO712G0Gs0y8",
        "outputId": "2cf8ab35-389b-4135-f44e-7843151756bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B disabled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect train model=yolov8n.pt data=/content/coco_dataset/data.yaml epochs=10 imgsz=640 batch=16 project=yolo_train name=exp wandb=False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA_YJs3gs3fT",
        "outputId": "1c0a7c37-2a18-4f64-dac0-96845a6b01cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/cfg/__init__.py\", line 385, in entrypoint\n",
            "    check_dict_alignment(full_args_dict, overrides)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/cfg/__init__.py\", line 203, in check_dict_alignment\n",
            "    raise SyntaxError(string + CLI_HELP_MSG) from e\n",
            "SyntaxError: '\u001b[31m\u001b[1mwandb\u001b[0m' is not a valid YOLO argument. \n",
            "\n",
            "    Arguments received: ['yolo', 'detect', 'train', 'model=yolov8n.pt', 'data=/content/coco_dataset/data.yaml', 'epochs=10', 'imgsz=640', 'batch=16', 'project=yolo_train', 'name=exp', 'wandb=False']. Ultralytics 'yolo' commands use the following syntax:\n",
            "\n",
            "        yolo TASK MODE ARGS\n",
            "\n",
            "        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n",
            "                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n",
            "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
            "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
            "\n",
            "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
            "        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n",
            "\n",
            "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
            "        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n",
            "\n",
            "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
            "        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n",
            "\n",
            "    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
            "        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n",
            "\n",
            "    5. Run special commands:\n",
            "        yolo help\n",
            "        yolo checks\n",
            "        yolo version\n",
            "        yolo settings\n",
            "        yolo copy-cfg\n",
            "        yolo cfg\n",
            "\n",
            "    Docs: https://docs.ultralytics.com\n",
            "    Community: https://community.ultralytics.com\n",
            "    GitHub: https://github.com/ultralytics/ultralytics\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "print(\"WANDB DISABLED\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naPD3dMvs4Op",
        "outputId": "31c44457-2e2f-4ceb-d243-f7eddb0988b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WANDB DISABLED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo settings wandb=False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFJ71ePhtBUe",
        "outputId": "6f4966ff-145c-476b-adc7-c3e612b9cb75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💡 Learn about settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings\n",
            "Printing '\u001b[1m\u001b[30m/root/.config/Ultralytics/settings.yaml\u001b[0m'\n",
            "\n",
            "settings_version: 0.0.4\n",
            "datasets_dir: /content/datasets\n",
            "weights_dir: weights\n",
            "runs_dir: runs\n",
            "uuid: 569f3ba64b326db489132663f79cd37279811de477381b83ac131e6cdd129cbb\n",
            "sync: true\n",
            "api_key: ''\n",
            "clearml: true\n",
            "comet: true\n",
            "dvc: true\n",
            "hub: true\n",
            "mlflow: true\n",
            "neptune: true\n",
            "raytune: true\n",
            "tensorboard: true\n",
            "wandb: false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "COCO_ROOT = Path(\"/content/coco_dataset\")\n",
        "TRAIN_JSON = COCO_ROOT / \"annotations/instances_train.json\"\n",
        "VAL_JSON = COCO_ROOT / \"annotations/instances_val.json\"\n",
        "\n",
        "LABEL_TRAIN = COCO_ROOT / \"labels/train\"\n",
        "LABEL_VAL = COCO_ROOT / \"labels/val\"\n",
        "\n",
        "LABEL_TRAIN.mkdir(parents=True, exist_ok=True)\n",
        "LABEL_VAL.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def coco_to_yolo(coco_json_path, label_output_path):\n",
        "    with open(coco_json_path, \"r\") as f:\n",
        "        coco = json.load(f)\n",
        "\n",
        "    images = {img[\"id\"]: img[\"file_name\"] for img in coco[\"images\"]}\n",
        "    image_sizes = {img[\"id\"]: (img[\"width\"], img[\"height\"]) for img in coco[\"images\"]}\n",
        "\n",
        "    anns = coco[\"annotations\"]\n",
        "\n",
        "    # clear directory\n",
        "    for txt in label_output_path.glob(\"*.txt\"):\n",
        "        txt.unlink()\n",
        "\n",
        "    for ann in anns:\n",
        "        img_id = ann[\"image_id\"]\n",
        "        file_name = images[img_id]\n",
        "        img_w, img_h = image_sizes[img_id]\n",
        "\n",
        "        # YOLO bbox format\n",
        "        x, y, w, h = ann[\"bbox\"]\n",
        "        xc = x + w / 2\n",
        "        yc = y + h / 2\n",
        "\n",
        "        # normalize\n",
        "        xc /= img_w\n",
        "        yc /= img_h\n",
        "        w /= img_w\n",
        "        h /= img_h\n",
        "\n",
        "        cls = ann[\"category_id\"] - 1  # YOLO class ids start at 0\n",
        "\n",
        "        txt_path = label_output_path / (file_name.replace('.png','').replace('.jpg','') + \".txt\")\n",
        "        with open(txt_path, \"a\") as f:\n",
        "            f.write(f\"{cls} {xc} {yc} {w} {h}\\n\")\n",
        "\n",
        "    print(f\"YOLO labels written to: {label_output_path}\")\n",
        "\n",
        "# Convert both splits\n",
        "coco_to_yolo(TRAIN_JSON, LABEL_TRAIN)\n",
        "coco_to_yolo(VAL_JSON, LABEL_VAL)\n",
        "\n",
        "print(\"Conversion complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgHcxNOFtDhn",
        "outputId": "041149da-1e80-499d-95f0-181862f6e56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO labels written to: /content/coco_dataset/labels/train\n",
            "YOLO labels written to: /content/coco_dataset/labels/val\n",
            "Conversion complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/coco_dataset/labels/train | head\n",
        "!ls /content/coco_dataset/labels/train | wc -l\n",
        "!ls /content/coco_dataset/labels/val | wc -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqm1PK_-tKyH",
        "outputId": "b7043fda-4871-414b-ccb3-afc1be8a639e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00000032_037.txt\n",
            "00000072_000.txt\n",
            "00000147_001.txt\n",
            "00000149_006.txt\n",
            "00000150_002.txt\n",
            "00000181_061.txt\n",
            "00000193_019.txt\n",
            "00000211_010.txt\n",
            "00000211_016.txt\n",
            "00000211_019.txt\n",
            "792\n",
            "88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/datasets/images-224/images-224\")\n",
        "sizes = set()\n",
        "\n",
        "for i, fname in enumerate(os.listdir(root)):\n",
        "    if i > 200:\n",
        "        break\n",
        "    img = Image.open(root / fname)\n",
        "    sizes.add(img.size)\n",
        "\n",
        "sizes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCel2ZjTtp-Q",
        "outputId": "0fee0d00-c015-43c8-b73f-0d854f99488b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(224, 224)}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "COCO_ROOT = Path(\"/content/coco_dataset\")\n",
        "TRAIN_JSON = COCO_ROOT / \"annotations/instances_train.json\"\n",
        "VAL_JSON   = COCO_ROOT / \"annotations/instances_val.json\"\n",
        "\n",
        "LABEL_TRAIN = COCO_ROOT / \"labels/train\"\n",
        "LABEL_VAL   = COCO_ROOT / \"labels/val\"\n",
        "\n",
        "LABEL_TRAIN.mkdir(parents=True, exist_ok=True)\n",
        "LABEL_VAL.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def convert_coco_to_yolo(json_file, label_dir, image_root):\n",
        "    with open(json_file, 'r') as f:\n",
        "        coco = json.load(f)\n",
        "\n",
        "    images = {img[\"id\"]: img for img in coco[\"images\"]}\n",
        "\n",
        "    # Clear old TXT files\n",
        "    for txt in label_dir.glob(\"*.txt\"):\n",
        "        txt.unlink()\n",
        "\n",
        "    for ann in coco[\"annotations\"]:\n",
        "        img_info = images[ann[\"image_id\"]]\n",
        "        img_file = img_info[\"file_name\"]\n",
        "\n",
        "        # Open image to read REAL width/height\n",
        "        img_path = image_root / img_file\n",
        "        try:\n",
        "            with Image.open(img_path) as im:\n",
        "                w_img, h_img = im.size\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        x, y, w, h = ann[\"bbox\"]\n",
        "\n",
        "        # YOLO format\n",
        "        xc = (x + w/2) / w_img\n",
        "        yc = (y + h/2) / h_img\n",
        "        wn = w / w_img\n",
        "        hn = h / h_img\n",
        "\n",
        "        # Skip invalid (YOLO requires all in 0-1 range)\n",
        "        if not (0 <= xc <= 1 and 0 <= yc <= 1 and 0 <= wn <= 1 and 0 <= hn <= 1):\n",
        "            continue\n",
        "\n",
        "        cls = ann[\"category_id\"] - 1\n",
        "\n",
        "        label_path = label_dir / f\"{img_file.rsplit('.',1)[0]}.txt\"\n",
        "        with open(label_path, \"a\") as f:\n",
        "            f.write(f\"{cls} {xc} {yc} {wn} {hn}\\n\")\n",
        "\n",
        "    print(f\"Converted: {json_file} → {label_dir}\")\n",
        "\n",
        "IMG_DIR = Path(\"/content/coco_dataset/images/train\")\n",
        "convert_coco_to_yolo(TRAIN_JSON, LABEL_TRAIN, IMG_DIR)\n",
        "\n",
        "IMG_DIR = Path(\"/content/coco_dataset/images/val\")\n",
        "convert_coco_to_yolo(VAL_JSON, LABEL_VAL, IMG_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqhOdLJBuTFM",
        "outputId": "355b62e1-0c98-44ee-cf89-621f689a0f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted: /content/coco_dataset/annotations/instances_train.json → /content/coco_dataset/labels/train\n",
            "Converted: /content/coco_dataset/annotations/instances_val.json → /content/coco_dataset/labels/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/coco_dataset/labels/train | wc -l\n",
        "!ls /content/coco_dataset/labels/val | wc -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAEe8-rguX1T",
        "outputId": "6b45e4b2-17e9-46ef-9681-d3ab4e3b08e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A - backup existing COCO and labels, set scale\n",
        "from pathlib import Path\n",
        "import shutil, json, os\n",
        "\n",
        "ROOT = Path(\"/content\")\n",
        "COCO_ORIG = ROOT / \"coco_chestxray.json\"\n",
        "COCO_SCALED = ROOT / \"coco_chestxray_scaled.json\"\n",
        "COCO_DATASET = ROOT / \"coco_dataset\"\n",
        "\n",
        "# Backup old files\n",
        "if COCO_ORIG.exists():\n",
        "    shutil.copy(str(COCO_ORIG), str(COCO_ORIG.with_suffix(\".bak.json\")))\n",
        "    print(\"Backed up original COCO to\", COCO_ORIG.with_suffix(\".bak.json\"))\n",
        "\n",
        "# Parameters\n",
        "ORIG_SIZE = 1024.0   # you told: take 1024 (original image size)\n",
        "NEW_SIZE = 224.0     # your current images are 224\n",
        "SCALE_X = NEW_SIZE / ORIG_SIZE\n",
        "SCALE_Y = NEW_SIZE / ORIG_SIZE\n",
        "print(\"Scale factors:\", SCALE_X, SCALE_Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUJgmAcZvg_A",
        "outputId": "cdadd42e-d8ad-4807-83f0-4622cf502a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backed up original COCO to /content/coco_chestxray.bak.json\n",
            "Scale factors: 0.21875 0.21875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell B - scale bboxes and write new COCO JSON that aligns to 224x224 images\n",
        "import json\n",
        "\n",
        "COCO_IN = \"/content/coco_chestxray.json\"\n",
        "COCO_OUT = \"/content/coco_chestxray_scaled.json\"\n",
        "\n",
        "ORIG_SIZE = 1024.0\n",
        "NEW_SIZE = 224.0\n",
        "SCALE_X = NEW_SIZE / ORIG_SIZE\n",
        "SCALE_Y = NEW_SIZE / ORIG_SIZE\n",
        "\n",
        "with open(COCO_IN, \"r\") as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "images = coco.get(\"images\", [])\n",
        "anns = coco.get(\"annotations\", [])\n",
        "cats = coco.get(\"categories\", [])\n",
        "\n",
        "new_images = []\n",
        "new_anns = []\n",
        "next_ann_id = 1\n",
        "\n",
        "# update image sizes to 224x224\n",
        "for img in images:\n",
        "    new_images.append({\n",
        "        \"id\": img['id'],\n",
        "        \"file_name\": img['file_name'],\n",
        "        \"width\": int(NEW_SIZE),\n",
        "        \"height\": int(NEW_SIZE)\n",
        "    })\n",
        "\n",
        "# scale and clip bounding boxes\n",
        "for ann in anns:\n",
        "    x, y, w, h = ann['bbox']\n",
        "\n",
        "    xs = x * SCALE_X\n",
        "    ys = y * SCALE_Y\n",
        "    ws = w * SCALE_X\n",
        "    hs = h * SCALE_Y\n",
        "\n",
        "    # clip to bounds\n",
        "    xs = max(0, min(xs, NEW_SIZE-1))\n",
        "    ys = max(0, min(ys, NEW_SIZE-1))\n",
        "    ws = max(1, min(ws, NEW_SIZE-xs))\n",
        "    hs = max(1, min(hs, NEW_SIZE-ys))\n",
        "\n",
        "    if ws < 1 or hs < 1:\n",
        "        continue\n",
        "\n",
        "    new_anns.append({\n",
        "        \"id\": next_ann_id,\n",
        "        \"image_id\": ann[\"image_id\"],\n",
        "        \"category_id\": ann[\"category_id\"],\n",
        "        \"bbox\": [xs, ys, ws, hs],\n",
        "        \"area\": float(ws * hs),\n",
        "        \"iscrowd\": ann.get(\"iscrowd\", 0)\n",
        "    })\n",
        "    next_ann_id += 1\n",
        "\n",
        "scaled_coco = {\n",
        "    \"images\": new_images,\n",
        "    \"annotations\": new_anns,\n",
        "    \"categories\": cats\n",
        "}\n",
        "\n",
        "with open(COCO_OUT, \"w\") as f:\n",
        "    json.dump(scaled_coco, f, indent=2)\n",
        "\n",
        "print(\"Saved SCALED COCO to:\", COCO_OUT)\n",
        "print(\"Images:\", len(new_images))\n",
        "print(\"Annotations:\", len(new_anns))\n",
        "print(\"Categories:\", len(cats))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANT_JwhXwkt8",
        "outputId": "761b9e3c-d9fc-49c8-ea97-678a619b926e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved SCALED COCO to: /content/coco_chestxray_scaled.json\n",
            "Images: 880\n",
            "Annotations: 984\n",
            "Categories: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell C - create coco_dataset (images already present) using SCALED COCO\n",
        "import json, random, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "COCO_SCALED = \"/content/coco_chestxray_scaled.json\"\n",
        "SRC_IMG_DIR = Path(\"/content/datasets/images-224/images-224\")\n",
        "\n",
        "OUT_ROOT = Path(\"/content/coco_dataset\")\n",
        "IMG_TRAIN = OUT_ROOT / \"images/train\"\n",
        "IMG_VAL = OUT_ROOT / \"images/val\"\n",
        "ANN_DIR = OUT_ROOT / \"annotations\"\n",
        "\n",
        "# make folders\n",
        "IMG_TRAIN.mkdir(parents=True, exist_ok=True)\n",
        "IMG_VAL.mkdir(parents=True, exist_ok=True)\n",
        "ANN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# load scaled COCO\n",
        "with open(COCO_SCALED, \"r\") as f:\n",
        "    coco = json.load(f)\n",
        "\n",
        "images = coco[\"images\"]\n",
        "annotations = coco[\"annotations\"]\n",
        "categories = coco[\"categories\"]\n",
        "\n",
        "random.seed(42)\n",
        "image_ids = [img[\"id\"] for img in images]\n",
        "random.shuffle(image_ids)\n",
        "\n",
        "split = int(0.90 * len(image_ids))   # 90% train\n",
        "train_ids = set(image_ids[:split])\n",
        "val_ids = set(image_ids[split:])\n",
        "\n",
        "train_images = [img for img in images if img[\"id\"] in train_ids]\n",
        "val_images   = [img for img in images if img[\"id\"] in val_ids]\n",
        "\n",
        "train_ann = [ann for ann in annotations if ann[\"image_id\"] in train_ids]\n",
        "val_ann   = [ann for ann in annotations if ann[\"image_id\"] in val_ids]\n",
        "\n",
        "missing = 0\n",
        "\n",
        "# copy images\n",
        "for img in images:\n",
        "    fname = img[\"file_name\"]\n",
        "    src = SRC_IMG_DIR / fname\n",
        "    if not src.exists():\n",
        "        missing += 1\n",
        "        continue\n",
        "    dst = IMG_TRAIN if img[\"id\"] in train_ids else IMG_VAL\n",
        "    shutil.copy(str(src), str(dst / fname))\n",
        "\n",
        "# write annotation files\n",
        "with open(ANN_DIR / \"instances_train.json\", \"w\") as f:\n",
        "    json.dump({\"images\": train_images, \"annotations\": train_ann, \"categories\": categories}, f, indent=2)\n",
        "\n",
        "with open(ANN_DIR / \"instances_val.json\", \"w\") as f:\n",
        "    json.dump({\"images\": val_images, \"annotations\": val_ann, \"categories\": categories}, f, indent=2)\n",
        "\n",
        "print(\"Train images:\", len(train_images))\n",
        "print(\"Val images:\", len(val_images))\n",
        "print(\"Train annotations:\", len(train_ann))\n",
        "print(\"Val annotations:\", len(val_ann))\n",
        "print(\"Missing images:\", missing)\n",
        "print(\"Scaled dataset created at:\", OUT_ROOT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvuCss3QwtdS",
        "outputId": "1eaf7a11-f64c-48ba-c093-ff3469d87a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 792\n",
            "Val images: 88\n",
            "Train annotations: 888\n",
            "Val annotations: 96\n",
            "Missing images: 0\n",
            "Scaled dataset created at: /content/coco_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell D - Convert scaled COCO annotations to YOLO TXT labels\n",
        "import json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "COCO_ROOT = Path(\"/content/coco_dataset\")\n",
        "TRAIN_JSON = COCO_ROOT / \"annotations/instances_train.json\"\n",
        "VAL_JSON   = COCO_ROOT / \"annotations/instances_val.json\"\n",
        "\n",
        "LABEL_TRAIN = COCO_ROOT / \"labels/train\"\n",
        "LABEL_VAL   = COCO_ROOT / \"labels/val\"\n",
        "\n",
        "LABEL_TRAIN.mkdir(parents=True, exist_ok=True)\n",
        "LABEL_VAL.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def convert_coco_to_yolo(json_file, label_output_path, image_root):\n",
        "    with open(json_file, \"r\") as f:\n",
        "        coco = json.load(f)\n",
        "\n",
        "    images = {img[\"id\"]: img for img in coco[\"images\"]}\n",
        "\n",
        "    # clear old txt files\n",
        "    for t in label_output_path.glob(\"*.txt\"):\n",
        "        t.unlink()\n",
        "\n",
        "    total_written = 0\n",
        "\n",
        "    for ann in coco[\"annotations\"]:\n",
        "        img_info = images[ann[\"image_id\"]]\n",
        "        img_file = img_info[\"file_name\"]\n",
        "\n",
        "        img_path = image_root / img_file\n",
        "\n",
        "        try:\n",
        "            with Image.open(img_path) as im:\n",
        "                w_img, h_img = im.size\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        x, y, w, h = ann[\"bbox\"]\n",
        "\n",
        "        # convert to YOLO format\n",
        "        xc = (x + w/2) / w_img\n",
        "        yc = (y + h/2) / h_img\n",
        "        wn = w / w_img\n",
        "        hn = h / h_img\n",
        "\n",
        "        # skip invalid normalized coords\n",
        "        if not (0 <= xc <= 1 and 0 <= yc <= 1 and 0 <= wn <= 1 and 0 <= hn <= 1):\n",
        "            continue\n",
        "\n",
        "        cls = ann[\"category_id\"] - 1\n",
        "\n",
        "        # write txt\n",
        "        txt_path = label_output_path / f\"{img_file.rsplit('.', 1)[0]}.txt\"\n",
        "        with open(txt_path, \"a\") as f:\n",
        "            f.write(f\"{cls} {xc:.6f} {yc:.6f} {wn:.6f} {hn:.6f}\\n\")\n",
        "        total_written += 1\n",
        "\n",
        "    print(f\"Converted {total_written} annotations to YOLO format in {label_output_path}\")\n",
        "\n",
        "# convert train\n",
        "convert_coco_to_yolo(\n",
        "    TRAIN_JSON,\n",
        "    LABEL_TRAIN,\n",
        "    COCO_ROOT / \"images/train\"\n",
        ")\n",
        "\n",
        "# convert val\n",
        "convert_coco_to_yolo(\n",
        "    VAL_JSON,\n",
        "    LABEL_VAL,\n",
        "    COCO_ROOT / \"images/val\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7WJgZWZw2un",
        "outputId": "966221c4-c644-4b11-ac2b-49e043bb0b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 888 annotations to YOLO format in /content/coco_dataset/labels/train\n",
            "Converted 96 annotations to YOLO format in /content/coco_dataset/labels/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/coco_dataset/labels/train | wc -l\n",
        "!ls /content/coco_dataset/labels/val | wc -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTWTXksfw_F7",
        "outputId": "b9a9a781-b5fb-48fa-c7fb-d5272d5afa76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "792\n",
            "88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect train model=yolov8n.pt data=/content/coco_dataset/data.yaml epochs=10 imgsz=640 batch=16 project=/content/yolo_train name=exp_final\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOqUgsYjxSQM",
        "outputId": "5942c872-5c72-445c-d272-c974d49f8b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "New https://pypi.org/project/ultralytics/8.3.228 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/coco_dataset/data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=/content/yolo_train, name=exp_final, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/content/yolo_train/exp_final\n",
            "2025-11-14 14:29:24.979209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763130564.999909   11170 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763130565.006408   11170 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763130565.022986   11170 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763130565.023015   11170 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763130565.023019   11170 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763130565.023022   11170 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3012408 parameters, 3012392 gradients\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/yolo_train/exp_final', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/checks.py:536: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py:244: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = amp.GradScaler(enabled=self.amp)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/coco_dataset/labels/train... 792 images, 0 backgrounds, 0 corrupt: 100% 792/792 [00:00<00:00, 2278.99it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/coco_dataset/labels/train.cache\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/data/augment.py:663: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=75, p=0.0)]  # transforms\n",
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/coco_dataset/labels/val... 88 images, 0 backgrounds, 0 corrupt: 100% 88/88 [00:00<00:00, 1152.06it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/coco_dataset/labels/val.cache\n",
            "Plotting labels to /content/yolo_train/exp_final/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/yolo_train/exp_final\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/data/augment.py:663: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=75, p=0.0)]  # transforms\n",
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/10      2.25G      2.612      6.092      2.648         10        640: 100% 50/50 [00:14<00:00,  3.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:02<00:00,  1.10it/s]\n",
            "                   all         88         96   0.000918      0.321      0.041     0.0172\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/10      2.17G       2.34      5.297       2.38          9        640: 100% 50/50 [00:12<00:00,  3.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.06it/s]\n",
            "                   all         88         96      0.909     0.0312     0.0562     0.0329\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/10      2.17G      2.348      5.022      2.341         10        640: 100% 50/50 [00:12<00:00,  4.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.94it/s]\n",
            "                   all         88         96      0.791     0.0625      0.058     0.0276\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/10      2.17G      2.282      4.756      2.274          9        640: 100% 50/50 [00:12<00:00,  4.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.69it/s]\n",
            "                   all         88         96      0.776      0.109      0.115     0.0707\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/10      2.17G      2.209      4.469      2.223          9        640: 100% 50/50 [00:12<00:00,  4.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.09it/s]\n",
            "                   all         88         96      0.888      0.109      0.133     0.0818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/10      2.17G      2.179      4.255      2.152          9        640: 100% 50/50 [00:12<00:00,  4.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.10it/s]\n",
            "                   all         88         96      0.824      0.127      0.153     0.0874\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/10      2.17G      2.129      4.088      2.152         12        640: 100% 50/50 [00:12<00:00,  4.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.26it/s]\n",
            "                   all         88         96      0.801      0.124      0.146     0.0842\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/10      2.17G      2.032      3.943       2.09          9        640: 100% 50/50 [00:12<00:00,  4.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.24it/s]\n",
            "                   all         88         96      0.859      0.137      0.185      0.107\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/10      2.17G      1.997      3.757      2.052          9        640: 100% 50/50 [00:12<00:00,  4.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.79it/s]\n",
            "                   all         88         96       0.81       0.15      0.189      0.106\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/10      2.17G       1.94      3.681      2.017         10        640: 100% 50/50 [00:12<00:00,  4.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.10it/s]\n",
            "                   all         88         96      0.776      0.188      0.209      0.121\n",
            "\n",
            "10 epochs completed in 0.042 hours.\n",
            "Optimizer stripped from /content/yolo_train/exp_final/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/yolo_train/exp_final/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/yolo_train/exp_final/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.99it/s]\n",
            "                   all         88         96      0.776      0.188      0.209      0.122\n",
            "           Atelectasis         88         21          1          0     0.0516     0.0246\n",
            "          Cardiomegaly         88         16      0.464          1      0.821      0.539\n",
            "              Effusion         88         12          1          0      0.026    0.00865\n",
            "            Infiltrate         88          7          1          0     0.0521     0.0197\n",
            "                  Mass         88          8      0.517       0.25      0.313      0.177\n",
            "                Nodule         88          8      0.227       0.25      0.161      0.111\n",
            "             Pneumonia         88         12          1          0      0.204      0.079\n",
            "          Pneumothorax         88         12          1          0     0.0458     0.0139\n",
            "Speed: 0.2ms preprocess, 2.2ms inference, 0.0ms loss, 5.2ms postprocess per image\n",
            "Results saved to \u001b[1m/content/yolo_train/exp_final\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mAP50      = 0.209\n",
        "mAP50-95   = 0.122\n",
        "Precision  = 0.776\n",
        "Recall     = 0.188\n"
      ],
      "metadata": {
        "id": "HQioa2JeyHoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_faster_rcnn.py\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------------------\n",
        "# CUSTOM COCO DATASET\n",
        "# ---------------------------\n",
        "class CocoDetection(Dataset):\n",
        "    def __init__(self, img_folder, ann_file, transforms=None):\n",
        "        self.coco = COCO(ann_file)\n",
        "        self.img_folder = img_folder\n",
        "        self.transforms = transforms\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        path = img_info[\"file_name\"]\n",
        "\n",
        "        # load image\n",
        "        img_path = os.path.join(self.img_folder, path)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # extract bounding boxes\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        areas = []\n",
        "        iscrowd = []\n",
        "\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            if w <= 0 or h <= 0:\n",
        "                continue\n",
        "\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(ann[\"category_id\"])\n",
        "            areas.append(ann[\"area\"])\n",
        "            iscrowd.append(ann.get(\"iscrowd\", 0))\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            # If no bbox exists, give dummy box (FRCNN NEEDS A BOX!)\n",
        "            boxes = [[0, 0, 1, 1]]\n",
        "            labels = [1]\n",
        "            areas = [1.0]\n",
        "            iscrowd = [0]\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "        iscrowd = torch.as_tensor(iscrowd)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = areas\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        target[\"image_id\"] = torch.tensor([img_id])\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# COLLATE FN\n",
        "# ---------------------------\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# EVALUATION FUNCTION\n",
        "# ---------------------------\n",
        "def evaluate(model, dataloader, ann_file, device):\n",
        "    print(\"Evaluating...\")\n",
        "    coco_gt = COCO(ann_file)\n",
        "\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in tqdm(dataloader):\n",
        "            imgs = [img.to(device) for img in imgs]\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            for target, output in zip(targets, outputs):\n",
        "                image_id = int(target[\"image_id\"].item())\n",
        "\n",
        "                if \"boxes\" not in output:\n",
        "                    continue\n",
        "\n",
        "                boxes = output[\"boxes\"].cpu().numpy()\n",
        "                scores = output[\"scores\"].cpu().numpy()\n",
        "                labels = output[\"labels\"].cpu().numpy()\n",
        "\n",
        "                for box, score, label in zip(boxes, scores, labels):\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    w = x2 - x1\n",
        "                    h = y2 - y1\n",
        "                    results.append({\n",
        "                        \"image_id\": image_id,\n",
        "                        \"category_id\": int(label),\n",
        "                        \"bbox\": [float(x1), float(y1), float(w), float(h)],\n",
        "                        \"score\": float(score)\n",
        "                    })\n",
        "\n",
        "    # Save temp JSON\n",
        "    import tempfile\n",
        "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".json\")\n",
        "    import json\n",
        "    json.dump(results, open(tmp.name, \"w\"))\n",
        "\n",
        "    coco_dt = coco_gt.loadRes(tmp.name)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "    return float(coco_eval.stats[1])  # mAP50\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# TRAINING LOOP\n",
        "# ---------------------------\n",
        "def train():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Paths\n",
        "    train_imgs = \"/content/coco_dataset/images/train\"\n",
        "    val_imgs = \"/content/coco_dataset/images/val\"\n",
        "    train_ann = \"/content/coco_dataset/annotations/instances_train.json\"\n",
        "    val_ann = \"/content/coco_dataset/annotations/instances_val.json\"\n",
        "\n",
        "    # Transforms\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "    # Dataset + Loader\n",
        "    train_dataset = CocoDetection(train_imgs, train_ann, transforms=transform)\n",
        "    val_dataset   = CocoDetection(val_imgs, val_ann, transforms=transform)\n",
        "\n",
        "    train_dl = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "    val_dl   = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Model\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.Adam(params, lr=0.0001)\n",
        "\n",
        "    best_map50 = 0.0\n",
        "\n",
        "    for epoch in range(1, 6):  # train 5 epochs\n",
        "        print(f\"\\n--- Epoch {epoch} ---\")\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for imgs, targets in tqdm(train_dl):\n",
        "            imgs = [img.to(device) for img in imgs]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(imgs, targets)\n",
        "            losses = sum(loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += losses.item()\n",
        "\n",
        "        print(f\"Epoch {epoch} Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # Validate\n",
        "        map50 = evaluate(model, val_dl, val_ann, device)\n",
        "        print(\"mAP50:\", map50)\n",
        "\n",
        "        if map50 > best_map50:\n",
        "            best_map50 = map50\n",
        "            torch.save(model.state_dict(), \"/content/faster_rcnn_best.pth\")\n",
        "            print(\"Saved NEW BEST model!\")\n",
        "\n",
        "    print(\"Training Completed.\")\n",
        "    print(\"Best mAP50:\", best_map50)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXZqAWU5yO1B",
        "outputId": "5db797be-2aa7-4d24-9f3f-b3e887fbac5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_faster_rcnn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_faster_rcnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZhzn5VYyUSB",
        "outputId": "26b77f8d-6a1c-4eaa-cea9-c6074e4973c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "\n",
            "--- Epoch 1 ---\n",
            "100% 198/198 [02:28<00:00,  1.33it/s]\n",
            "Epoch 1 Loss: 52.1369\n",
            "Evaluating...\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "100% 22/22 [00:07<00:00,  2.98it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_faster_rcnn.py\", line 203, in <module>\n",
            "    train()\n",
            "  File \"/content/train_faster_rcnn.py\", line 190, in train\n",
            "    map50 = evaluate(model, val_dl, val_ann, device)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/train_faster_rcnn.py\", line 127, in evaluate\n",
            "    coco_dt = coco_gt.loadRes(tmp.name)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pycocotools/coco.py\", line 314, in loadRes\n",
            "    res.dataset['info'] = copy.deepcopy(self.dataset['info'])\n",
            "                                        ~~~~~~~~~~~~^^^^^^^^\n",
            "KeyError: 'info'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"/content/yolo_train/exp_final/weights/best.pt\")\n",
        "metrics = model.val(data=\"/content/coco_dataset/data.yaml\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njbV9Nj_yyU2",
        "outputId": "9ad5b290-660d-4191-a22e-772a6e21221f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/coco_dataset/labels/val.cache... 88 images, 0 backgrounds, 0 corrupt: 100%|██████████| 88/88 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:01<00:00,  3.20it/s]\n",
            "                   all         88         96      0.775      0.188      0.209      0.122\n",
            "           Atelectasis         88         21          1          0     0.0516     0.0246\n",
            "          Cardiomegaly         88         16      0.465          1       0.82      0.538\n",
            "              Effusion         88         12          1          0     0.0257    0.00859\n",
            "            Infiltrate         88          7          1          0     0.0517     0.0195\n",
            "                  Mass         88          8      0.513       0.25      0.313      0.176\n",
            "                Nodule         88          8      0.225       0.25      0.164      0.113\n",
            "             Pneumonia         88         12          1          0      0.205     0.0802\n",
            "          Pneumothorax         88         12          1          0     0.0458     0.0139\n",
            "Speed: 3.6ms preprocess, 4.5ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo val model=/content/yolo_train/exp_final/weights/best.pt data=/content/coco_dataset/data.yaml imgsz=640\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt5c3F_py1bA",
        "outputId": "b2320848-333f-48f5-cff2-9ae70d982254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/coco_dataset/labels/val.cache... 88 images, 0 backgrounds, 0 corrupt: 100% 88/88 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.23it/s]\n",
            "                   all         88         96      0.775      0.188      0.209      0.122\n",
            "           Atelectasis         88         21          1          0     0.0516     0.0246\n",
            "          Cardiomegaly         88         16      0.465          1       0.82      0.538\n",
            "              Effusion         88         12          1          0     0.0257    0.00859\n",
            "            Infiltrate         88          7          1          0     0.0517     0.0195\n",
            "                  Mass         88          8      0.513       0.25      0.313      0.176\n",
            "                Nodule         88          8      0.225       0.25      0.164      0.113\n",
            "             Pneumonia         88         12          1          0      0.205     0.0802\n",
            "          Pneumothorax         88         12          1          0     0.0458     0.0139\n",
            "Speed: 4.9ms preprocess, 5.0ms inference, 0.0ms loss, 3.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val3\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect predict model=/content/yolo_train/exp_final/weights/best.pt source=/content/input.png conf=0.25 save=True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfhvmKB_y6t7",
        "outputId": "1f63d5bb-a769-4f7d-998b-f78a8d163213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/cfg/__init__.py\", line 445, in entrypoint\n",
            "    getattr(model, mode)(**overrides)  # default args from model\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\", line 236, in predict\n",
            "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/predictor.py\", line 199, in predict_cli\n",
            "    for _ in gen:  # running CLI inference without accumulating any outputs (do not modify)\n",
            "             ^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/predictor.py\", line 229, in stream_inference\n",
            "    self.setup_source(source if source is not None else self.args.source)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/predictor.py\", line 207, in setup_source\n",
            "    self.dataset = load_inference_source(source=source,\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/data/build.py\", line 166, in load_inference_source\n",
            "    dataset = LoadImages(source, imgsz=imgsz, vid_stride=vid_stride)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/data/loaders.py\", line 215, in __init__\n",
            "    raise FileNotFoundError(f'{p} does not exist')\n",
            "FileNotFoundError: /content/input.png does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect predict model=/content/yolo_train/exp_final/weights/best.pt source=/content/coco_dataset/images/val save=True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck4Dyszmy9a3",
        "outputId": "eb2d3159-ac79-42fe-a3a8-c6ad31f4027d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "\n",
            "image 1/88 /content/coco_dataset/images/val/00000457_004.png: 640x640 1 Cardiomegaly, 12.2ms\n",
            "image 2/88 /content/coco_dataset/images/val/00000661_000.png: 640x640 1 Cardiomegaly, 7.5ms\n",
            "image 3/88 /content/coco_dataset/images/val/00000744_006.png: 640x640 (no detections), 8.0ms\n",
            "image 4/88 /content/coco_dataset/images/val/00001075_024.png: 640x640 (no detections), 7.4ms\n",
            "image 5/88 /content/coco_dataset/images/val/00001369_000.png: 640x640 1 Cardiomegaly, 7.4ms\n",
            "image 6/88 /content/coco_dataset/images/val/00001900_026.png: 640x640 (no detections), 7.3ms\n",
            "image 7/88 /content/coco_dataset/images/val/00002704_029.png: 640x640 1 Cardiomegaly, 7.4ms\n",
            "image 8/88 /content/coco_dataset/images/val/00004344_013.png: 640x640 1 Cardiomegaly, 7.4ms\n",
            "image 9/88 /content/coco_dataset/images/val/00004808_090.png: 640x640 (no detections), 11.3ms\n",
            "image 10/88 /content/coco_dataset/images/val/00005140_001.png: 640x640 (no detections), 11.0ms\n",
            "image 11/88 /content/coco_dataset/images/val/00005532_014.png: 640x640 1 Cardiomegaly, 10.3ms\n",
            "image 12/88 /content/coco_dataset/images/val/00006751_000.png: 640x640 1 Cardiomegaly, 9.4ms\n",
            "image 13/88 /content/coco_dataset/images/val/00008008_027.png: 640x640 (no detections), 7.7ms\n",
            "image 14/88 /content/coco_dataset/images/val/00008365_000.png: 640x640 1 Cardiomegaly, 10.4ms\n",
            "image 15/88 /content/coco_dataset/images/val/00008522_032.png: 640x640 1 Cardiomegaly, 11.0ms\n",
            "image 16/88 /content/coco_dataset/images/val/00009229_003.png: 640x640 (no detections), 12.5ms\n",
            "image 17/88 /content/coco_dataset/images/val/00009619_000.png: 640x640 (no detections), 8.1ms\n",
            "image 18/88 /content/coco_dataset/images/val/00011151_004.png: 640x640 (no detections), 7.4ms\n",
            "image 19/88 /content/coco_dataset/images/val/00011263_004.png: 640x640 1 Cardiomegaly, 7.3ms\n",
            "image 20/88 /content/coco_dataset/images/val/00011322_002.png: 640x640 1 Cardiomegaly, 11.5ms\n",
            "image 21/88 /content/coco_dataset/images/val/00011514_015.png: 640x640 1 Cardiomegaly, 7.4ms\n",
            "image 22/88 /content/coco_dataset/images/val/00012048_007.png: 640x640 1 Cardiomegaly, 8.0ms\n",
            "image 23/88 /content/coco_dataset/images/val/00012174_000.png: 640x640 1 Cardiomegaly, 7.3ms\n",
            "image 24/88 /content/coco_dataset/images/val/00012374_000.png: 640x640 (no detections), 8.1ms\n",
            "image 25/88 /content/coco_dataset/images/val/00012515_002.png: 640x640 (no detections), 7.5ms\n",
            "image 26/88 /content/coco_dataset/images/val/00012670_000.png: 640x640 1 Cardiomegaly, 7.4ms\n",
            "image 27/88 /content/coco_dataset/images/val/00013111_069.png: 640x640 (no detections), 7.3ms\n",
            "image 28/88 /content/coco_dataset/images/val/00013685_028.png: 640x640 (no detections), 9.4ms\n",
            "image 29/88 /content/coco_dataset/images/val/00014251_029.png: 640x640 (no detections), 11.2ms\n",
            "image 30/88 /content/coco_dataset/images/val/00014294_011.png: 640x640 (no detections), 8.1ms\n",
            "image 31/88 /content/coco_dataset/images/val/00014731_028.png: 640x640 1 Mass, 2 Nodules, 11.0ms\n",
            "image 32/88 /content/coco_dataset/images/val/00015069_001.png: 640x640 1 Cardiomegaly, 11.6ms\n",
            "image 33/88 /content/coco_dataset/images/val/00015300_000.png: 640x640 (no detections), 9.7ms\n",
            "image 34/88 /content/coco_dataset/images/val/00015649_000.png: 640x640 (no detections), 7.4ms\n",
            "image 35/88 /content/coco_dataset/images/val/00015770_010.png: 640x640 1 Cardiomegaly, 9.3ms\n",
            "image 36/88 /content/coco_dataset/images/val/00015895_017.png: 640x640 (no detections), 8.0ms\n",
            "image 37/88 /content/coco_dataset/images/val/00016191_017.png: 640x640 (no detections), 6.8ms\n",
            "image 38/88 /content/coco_dataset/images/val/00016291_002.png: 640x640 (no detections), 9.5ms\n",
            "image 39/88 /content/coco_dataset/images/val/00016291_012.png: 640x640 1 Cardiomegaly, 10.1ms\n",
            "image 40/88 /content/coco_dataset/images/val/00016417_008.png: 640x640 (no detections), 8.9ms\n",
            "image 41/88 /content/coco_dataset/images/val/00016487_002.png: 640x640 1 Cardiomegaly, 9.5ms\n",
            "image 42/88 /content/coco_dataset/images/val/00016964_011.png: 640x640 (no detections), 10.3ms\n",
            "image 43/88 /content/coco_dataset/images/val/00017138_037.png: 640x640 (no detections), 9.3ms\n",
            "image 44/88 /content/coco_dataset/images/val/00017199_005.png: 640x640 (no detections), 9.7ms\n",
            "image 45/88 /content/coco_dataset/images/val/00017243_010.png: 640x640 1 Nodule, 9.1ms\n",
            "image 46/88 /content/coco_dataset/images/val/00017511_006.png: 640x640 1 Cardiomegaly, 9.2ms\n",
            "image 47/88 /content/coco_dataset/images/val/00017524_028.png: 640x640 (no detections), 9.4ms\n",
            "image 48/88 /content/coco_dataset/images/val/00017893_005.png: 640x640 1 Cardiomegaly, 10.3ms\n",
            "image 49/88 /content/coco_dataset/images/val/00017952_008.png: 640x640 (no detections), 11.0ms\n",
            "image 50/88 /content/coco_dataset/images/val/00018055_038.png: 640x640 (no detections), 7.5ms\n",
            "image 51/88 /content/coco_dataset/images/val/00018055_045.png: 640x640 (no detections), 7.6ms\n",
            "image 52/88 /content/coco_dataset/images/val/00018253_054.png: 640x640 (no detections), 6.8ms\n",
            "image 53/88 /content/coco_dataset/images/val/00018496_007.png: 640x640 (no detections), 6.7ms\n",
            "image 54/88 /content/coco_dataset/images/val/00018865_008.png: 640x640 (no detections), 6.8ms\n",
            "image 55/88 /content/coco_dataset/images/val/00019124_045.png: 640x640 (no detections), 7.8ms\n",
            "image 56/88 /content/coco_dataset/images/val/00019157_008.png: 640x640 (no detections), 6.8ms\n",
            "image 57/88 /content/coco_dataset/images/val/00019750_012.png: 640x640 (no detections), 6.8ms\n",
            "image 58/88 /content/coco_dataset/images/val/00019924_020.png: 640x640 (no detections), 6.8ms\n",
            "image 59/88 /content/coco_dataset/images/val/00020113_017.png: 640x640 (no detections), 6.8ms\n",
            "image 60/88 /content/coco_dataset/images/val/00020124_003.png: 640x640 (no detections), 6.7ms\n",
            "image 61/88 /content/coco_dataset/images/val/00020408_037.png: 640x640 (no detections), 6.8ms\n",
            "image 62/88 /content/coco_dataset/images/val/00020408_058.png: 640x640 (no detections), 6.8ms\n",
            "image 63/88 /content/coco_dataset/images/val/00020671_010.png: 640x640 (no detections), 6.7ms\n",
            "image 64/88 /content/coco_dataset/images/val/00020774_000.png: 640x640 (no detections), 6.8ms\n",
            "image 65/88 /content/coco_dataset/images/val/00021024_022.png: 640x640 1 Cardiomegaly, 6.7ms\n",
            "image 66/88 /content/coco_dataset/images/val/00022572_073.png: 640x640 (no detections), 6.8ms\n",
            "image 67/88 /content/coco_dataset/images/val/00022961_008.png: 640x640 (no detections), 6.8ms\n",
            "image 68/88 /content/coco_dataset/images/val/00023116_005.png: 640x640 (no detections), 5.8ms\n",
            "image 69/88 /content/coco_dataset/images/val/00023156_001.png: 640x640 (no detections), 5.5ms\n",
            "image 70/88 /content/coco_dataset/images/val/00023283_019.png: 640x640 (no detections), 5.5ms\n",
            "image 71/88 /content/coco_dataset/images/val/00025221_001.png: 640x640 (no detections), 7.6ms\n",
            "image 72/88 /content/coco_dataset/images/val/00025252_040.png: 640x640 (no detections), 5.5ms\n",
            "image 73/88 /content/coco_dataset/images/val/00025707_015.png: 640x640 (no detections), 5.5ms\n",
            "image 74/88 /content/coco_dataset/images/val/00025962_000.png: 640x640 (no detections), 5.5ms\n",
            "image 75/88 /content/coco_dataset/images/val/00026392_005.png: 640x640 (no detections), 5.7ms\n",
            "image 76/88 /content/coco_dataset/images/val/00026810_001.png: 640x640 (no detections), 5.5ms\n",
            "image 77/88 /content/coco_dataset/images/val/00027631_000.png: 640x640 (no detections), 5.5ms\n",
            "image 78/88 /content/coco_dataset/images/val/00027652_003.png: 640x640 (no detections), 5.5ms\n",
            "image 79/88 /content/coco_dataset/images/val/00027837_001.png: 640x640 (no detections), 7.4ms\n",
            "image 80/88 /content/coco_dataset/images/val/00028012_001.png: 640x640 (no detections), 9.8ms\n",
            "image 81/88 /content/coco_dataset/images/val/00028173_016.png: 640x640 (no detections), 6.5ms\n",
            "image 82/88 /content/coco_dataset/images/val/00028607_000.png: 640x640 1 Cardiomegaly, 5.5ms\n",
            "image 83/88 /content/coco_dataset/images/val/00029259_027.png: 640x640 (no detections), 5.5ms\n",
            "image 84/88 /content/coco_dataset/images/val/00029940_007.png: 640x640 (no detections), 5.8ms\n",
            "image 85/88 /content/coco_dataset/images/val/00030162_029.png: 640x640 (no detections), 7.3ms\n",
            "image 86/88 /content/coco_dataset/images/val/00030260_004.png: 640x640 (no detections), 7.4ms\n",
            "image 87/88 /content/coco_dataset/images/val/00030260_005.png: 640x640 (no detections), 5.7ms\n",
            "image 88/88 /content/coco_dataset/images/val/00030279_000.png: 640x640 1 Cardiomegaly, 5.4ms\n",
            "Speed: 2.5ms preprocess, 7.9ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect train model=yolov8n.pt data=/content/coco_dataset/data.yaml epochs=50 imgsz=640 batch=16 project=yolo_train name=exp_long\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w0KKCgG0s2x",
        "outputId": "3a72b74e-8c13-4096-ba4b-558a50a05cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "New https://pypi.org/project/ultralytics/8.3.228 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/coco_dataset/data.yaml, epochs=50, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=yolo_train, name=exp_long, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=yolo_train/exp_long\n",
            "2025-11-14 14:46:10.038698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763131570.071379   15598 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763131570.081438   15598 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763131570.105978   15598 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763131570.106023   15598 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763131570.106031   15598 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763131570.106038   15598 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3012408 parameters, 3012392 gradients\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolo_train/exp_long', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/checks.py:536: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(True):\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py:244: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = amp.GradScaler(enabled=self.amp)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/coco_dataset/labels/train.cache... 792 images, 0 backgrounds, 0 corrupt: 100% 792/792 [00:00<?, ?it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/data/augment.py:663: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=75, p=0.0)]  # transforms\n",
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/coco_dataset/labels/val.cache... 88 images, 0 backgrounds, 0 corrupt: 100% 88/88 [00:00<?, ?it/s]\n",
            "Plotting labels to yolo_train/exp_long/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1myolo_train/exp_long\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/50      2.25G      2.505      5.293      2.386         18        640: 100% 50/50 [00:15<00:00,  3.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.77it/s]\n",
            "                   all         88         96    0.00109      0.329     0.0366     0.0133\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/50      2.16G      2.293      4.745      2.123         15        640: 100% 50/50 [00:14<00:00,  3.50it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.76it/s]\n",
            "                   all         88         96      0.922    0.00781     0.0268     0.0116\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/50      2.26G      2.238      4.482      2.128         19        640: 100% 50/50 [00:12<00:00,  3.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.60it/s]\n",
            "                   all         88         96      0.535     0.0781     0.0372     0.0202\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/50      2.25G       2.22      4.363      2.143         12        640: 100% 50/50 [00:13<00:00,  3.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.21it/s]\n",
            "                   all         88         96      0.908     0.0625     0.0886     0.0375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/50      2.26G      2.193      4.175      2.131         14        640: 100% 50/50 [00:12<00:00,  3.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.68it/s]\n",
            "                   all         88         96      0.831      0.125     0.0876     0.0415\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/50      2.23G       2.22      4.051      2.161         21        640: 100% 50/50 [00:13<00:00,  3.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.91it/s]\n",
            "                   all         88         96      0.805     0.0625     0.0901     0.0471\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/50      2.24G      2.157       3.93      2.115         15        640: 100% 50/50 [00:13<00:00,  3.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.97it/s]\n",
            "                   all         88         96      0.798      0.158      0.124      0.058\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/50      2.24G      2.148      3.818       2.04         18        640: 100% 50/50 [00:13<00:00,  3.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.15it/s]\n",
            "                   all         88         96       0.71     0.0938     0.0936     0.0501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/50      2.24G      2.046      3.636      1.986         16        640: 100% 50/50 [00:13<00:00,  3.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.86it/s]\n",
            "                   all         88         96      0.782      0.133      0.111     0.0511\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/50      2.25G      2.061      3.622      2.024          6        640: 100% 50/50 [00:13<00:00,  3.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.91it/s]\n",
            "                   all         88         96      0.946     0.0859      0.124     0.0653\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/50      2.24G      2.042      3.541      1.993         18        640: 100% 50/50 [00:13<00:00,  3.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.98it/s]\n",
            "                   all         88         96      0.605     0.0949      0.166     0.0895\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/50      2.23G      2.017       3.45      1.985         14        640: 100% 50/50 [00:13<00:00,  3.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.23it/s]\n",
            "                   all         88         96       0.78      0.113      0.135     0.0724\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/50      2.26G       1.98      3.377      1.947         13        640: 100% 50/50 [00:13<00:00,  3.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.80it/s]\n",
            "                   all         88         96      0.638      0.214      0.163     0.0826\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/50      2.24G      1.947      3.285       1.94         10        640: 100% 50/50 [00:13<00:00,  3.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.34it/s]\n",
            "                   all         88         96      0.739       0.11      0.116      0.066\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/50      2.24G      1.954      3.316      1.918          9        640: 100% 50/50 [00:13<00:00,  3.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.35it/s]\n",
            "                   all         88         96      0.778      0.109       0.14      0.083\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/50      2.25G      1.936      3.222      1.909          9        640: 100% 50/50 [00:13<00:00,  3.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.30it/s]\n",
            "                   all         88         96      0.258      0.174       0.16     0.0857\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/50      2.24G      1.894      3.211      1.894         18        640: 100% 50/50 [00:13<00:00,  3.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.10it/s]\n",
            "                   all         88         96      0.627      0.165      0.186     0.0983\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/50      2.25G      1.884      3.072      1.876         20        640: 100% 50/50 [00:13<00:00,  3.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.02it/s]\n",
            "                   all         88         96      0.941      0.102      0.162        0.1\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/50      2.26G      1.906      3.151      1.904         12        640: 100% 50/50 [00:13<00:00,  3.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.45it/s]\n",
            "                   all         88         96      0.641      0.168      0.187      0.104\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/50      2.25G      1.844          3      1.839         18        640: 100% 50/50 [00:13<00:00,  3.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.24it/s]\n",
            "                   all         88         96       0.79      0.219      0.203      0.106\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/50      2.24G      1.886      2.984      1.841         12        640: 100% 50/50 [00:13<00:00,  3.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.43it/s]\n",
            "                   all         88         96       0.38      0.234      0.189     0.0962\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/50      2.24G      1.846      2.983      1.823         11        640: 100% 50/50 [00:13<00:00,  3.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.11it/s]\n",
            "                   all         88         96      0.383      0.195      0.193      0.091\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/50      2.24G      1.826      2.964       1.84         14        640: 100% 50/50 [00:13<00:00,  3.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.25it/s]\n",
            "                   all         88         96      0.939      0.156       0.18        0.1\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/50      2.24G      1.814      2.918      1.825         12        640: 100% 50/50 [00:13<00:00,  3.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.33it/s]\n",
            "                   all         88         96      0.665      0.148      0.175     0.0961\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/50      2.24G      1.832      2.876      1.841         11        640: 100% 50/50 [00:13<00:00,  3.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.87it/s]\n",
            "                   all         88         96      0.504       0.18      0.178        0.1\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/50      2.23G       1.75      2.763       1.75         20        640: 100% 50/50 [00:13<00:00,  3.71it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.76it/s]\n",
            "                   all         88         96       0.24      0.201      0.146     0.0774\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/50      2.24G       1.75      2.776      1.759         12        640: 100% 50/50 [00:13<00:00,  3.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.81it/s]\n",
            "                   all         88         96      0.341      0.232      0.172     0.0939\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/50      2.25G      1.749      2.774      1.792          8        640: 100% 50/50 [00:13<00:00,  3.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.84it/s]\n",
            "                   all         88         96      0.792      0.141      0.168     0.0902\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/50      2.26G      1.707      2.721      1.741         11        640: 100% 50/50 [00:13<00:00,  3.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.45it/s]\n",
            "                   all         88         96      0.522      0.203      0.189      0.103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      30/50      2.25G      1.734      2.693      1.755         21        640: 100% 50/50 [00:13<00:00,  3.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.39it/s]\n",
            "                   all         88         96      0.544      0.206      0.191      0.102\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      31/50      2.24G       1.73      2.661      1.744         15        640: 100% 50/50 [00:13<00:00,  3.71it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.13it/s]\n",
            "                   all         88         96      0.198      0.285       0.21      0.113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      32/50      2.23G      1.665       2.63      1.721         20        640: 100% 50/50 [00:13<00:00,  3.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.00it/s]\n",
            "                   all         88         96      0.658      0.201      0.202      0.115\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      33/50      2.24G      1.689       2.62      1.715         18        640: 100% 50/50 [00:13<00:00,  3.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.52it/s]\n",
            "                   all         88         96      0.165       0.24      0.183      0.101\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      34/50      2.24G      1.673      2.588      1.698         10        640: 100% 50/50 [00:13<00:00,  3.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.88it/s]\n",
            "                   all         88         96      0.162      0.239      0.206      0.109\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      35/50      2.26G      1.672      2.584      1.713         12        640: 100% 50/50 [00:13<00:00,  3.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.86it/s]\n",
            "                   all         88         96      0.138      0.235      0.201      0.113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      36/50      2.25G      1.621      2.497       1.67         12        640: 100% 50/50 [00:13<00:00,  3.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.92it/s]\n",
            "                   all         88         96      0.173      0.254      0.188      0.106\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      37/50      2.24G      1.618      2.489      1.683         11        640: 100% 50/50 [00:13<00:00,  3.79it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.67it/s]\n",
            "                   all         88         96      0.172      0.299      0.216      0.109\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      38/50      2.23G      1.603      2.515      1.653         14        640: 100% 50/50 [00:13<00:00,  3.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.38it/s]\n",
            "                   all         88         96      0.211      0.304      0.208      0.116\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      39/50      2.24G      1.576       2.38      1.636         17        640: 100% 50/50 [00:13<00:00,  3.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.18it/s]\n",
            "                   all         88         96      0.298      0.244      0.193      0.108\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      40/50      2.23G      1.556      2.383      1.616         13        640: 100% 50/50 [00:13<00:00,  3.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.31it/s]\n",
            "                   all         88         96      0.216       0.28      0.194        0.1\n",
            "Closing dataloader mosaic\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/data/augment.py:663: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=75, p=0.0)]  # transforms\n",
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      41/50      2.24G       1.58       2.34      1.763          8        640: 100% 50/50 [00:14<00:00,  3.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.52it/s]\n",
            "                   all         88         96      0.243      0.277      0.206      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      42/50      2.23G      1.551      2.277      1.738          8        640: 100% 50/50 [00:12<00:00,  3.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.66it/s]\n",
            "                   all         88         96      0.186      0.297       0.21      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      43/50      2.24G        1.5      2.221      1.716          8        640: 100% 50/50 [00:12<00:00,  4.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.32it/s]\n",
            "                   all         88         96      0.215      0.288      0.209      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      44/50      2.23G      1.482       2.18      1.709          8        640: 100% 50/50 [00:12<00:00,  3.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  2.54it/s]\n",
            "                   all         88         96      0.148      0.278      0.206      0.104\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      45/50      2.24G      1.483      2.119      1.679         10        640: 100% 50/50 [00:12<00:00,  3.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.20it/s]\n",
            "                   all         88         96      0.226      0.318      0.221      0.116\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      46/50      2.23G      1.454      2.095      1.674         10        640: 100% 50/50 [00:12<00:00,  3.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.29it/s]\n",
            "                   all         88         96      0.188      0.321      0.206      0.113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      47/50      2.24G      1.445      2.076      1.667          7        640: 100% 50/50 [00:12<00:00,  3.98it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.13it/s]\n",
            "                   all         88         96      0.258      0.289      0.228       0.12\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      48/50      2.23G      1.414      2.048      1.648          7        640: 100% 50/50 [00:12<00:00,  3.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.54it/s]\n",
            "                   all         88         96      0.247      0.329      0.222      0.112\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      49/50      2.24G      1.397      2.015      1.631          8        640: 100% 50/50 [00:12<00:00,  4.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.05it/s]\n",
            "                   all         88         96      0.218      0.292      0.208      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      50/50      2.23G      1.377      1.981      1.617          9        640: 100% 50/50 [00:12<00:00,  3.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:00<00:00,  3.35it/s]\n",
            "                   all         88         96      0.204      0.298      0.214      0.118\n",
            "\n",
            "50 epochs completed in 0.214 hours.\n",
            "Optimizer stripped from yolo_train/exp_long/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from yolo_train/exp_long/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating yolo_train/exp_long/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:01<00:00,  1.64it/s]\n",
            "                   all         88         96      0.239      0.292      0.228      0.119\n",
            "           Atelectasis         88         21       0.54     0.0476     0.0732     0.0242\n",
            "          Cardiomegaly         88         16      0.592          1      0.896      0.597\n",
            "              Effusion         88         12     0.0916     0.0833     0.0407     0.0116\n",
            "            Infiltrate         88          7          0          0     0.0259     0.0106\n",
            "                  Mass         88          8       0.21        0.5      0.441      0.174\n",
            "                Nodule         88          8      0.145      0.375     0.0933     0.0375\n",
            "             Pneumonia         88         12      0.157       0.25      0.146     0.0593\n",
            "          Pneumothorax         88         12      0.178     0.0833      0.109      0.036\n",
            "Speed: 0.3ms preprocess, 2.7ms inference, 0.0ms loss, 5.4ms postprocess per image\n",
            "Results saved to \u001b[1myolo_train/exp_long\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KTUkxlBM-GgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model=/content/yolo_train/exp_final/weights/best.pt format=onnx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyxk3qv90wEO",
        "outputId": "756b33f7-16b4-4c52-97b9-1a6439b08ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/yolo_train/exp_final/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 12, 8400) (6.0 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.12.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.12.0) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.12.0) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.12.0) (0.5.3)\n",
            "Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 219.9 MB/s eta 0:00:00\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.19.1\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 6.3s, installed 1 package: ['onnx>=1.12.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 6.9s, saved as '/content/yolo_train/exp_final/weights/best.onnx' (11.7 MB)\n",
            "\n",
            "Export complete (9.0s)\n",
            "Results saved to \u001b[1m/content/yolo_train/exp_final/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=/content/yolo_train/exp_final/weights/best.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=/content/yolo_train/exp_final/weights/best.onnx imgsz=640 data=/content/coco_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model=/content/yolo_train/exp_final/weights/best.pt format=engine\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUmuchlA0zdO",
        "outputId": "00863192-3558-40fb-a1c0-f7e6e9282183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "WARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\n",
            "Ultralytics YOLOv8.0.185 🚀 Python-3.12.12 torch-2.4.0+cu118 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/yolo_train/exp_final/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 12, 8400) (6.0 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['nvidia-tensorrt'] not found, attempting AutoUpdate...\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement nvidia-tensorrt (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for nvidia-tensorrt\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31m\u001b[1mrequirements:\u001b[0m ❌ Command 'pip install --no-cache \"nvidia-tensorrt\" -U --index-url https://pypi.ngc.nvidia.com' returned non-zero exit status 1.\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export failure ❌ 7.2s: No module named 'tensorrt'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/exporter.py\", line 581, in export_engine\n",
            "    import tensorrt as trt  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'tensorrt'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/cfg/__init__.py\", line 445, in entrypoint\n",
            "    getattr(model, mode)(**overrides)  # default args from model\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\", line 309, in export\n",
            "    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/exporter.py\", line 252, in __call__\n",
            "    f[1], _ = self.export_engine()\n",
            "              ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/exporter.py\", line 122, in outer_func\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/exporter.py\", line 117, in outer_func\n",
            "    f, model = inner_func(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/exporter.py\", line 585, in export_engine\n",
            "    import tensorrt as trt  # noqa\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'tensorrt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO(\"best.pt\")\n",
        "results = model(\"xray.png\")\n",
        "results.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "YXxF4pIO01U8",
        "outputId": "191481b5-e545-451c-cd66-a56ae631bd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:527: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'best.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2058894524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xray.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_ckpt_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;34m\"\"\"Loads a single model weights.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m     \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_safe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mDEFAULT_CFG_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# combine model and default args, preferring model args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ema'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0;34m'ultralytics.yolo.v8'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ultralytics.models.yolo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 'ultralytics.yolo.data': 'ultralytics.data'}):  # for legacy 8.0 Classify and Pose models\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m  \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# e.name is missing module name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/yolo_train/exp_final/weights/best.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ILgkxShB03fV",
        "outputId": "f1f2c308-4eb4-4a70-d0e2-04454d1e5897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b14604f7-fc42-425a-812e-c15cbde63885\", \"best.pt\", 6249177)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def fix_coco(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        coco = json.load(f)\n",
        "\n",
        "    # Add required COCO fields if missing\n",
        "    if \"info\" not in coco:\n",
        "        coco[\"info\"] = {\n",
        "            \"description\": \"NIH Chest X-ray Detection Dataset\",\n",
        "            \"version\": \"1.0\",\n",
        "            \"year\": 2025\n",
        "        }\n",
        "\n",
        "    if \"licenses\" not in coco:\n",
        "        coco[\"licenses\"] = [{\n",
        "            \"id\": 1,\n",
        "            \"name\": \"NIH License\",\n",
        "            \"url\": \"https://nih.gov\"\n",
        "        }]\n",
        "\n",
        "    # save back\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(coco, f, indent=2)\n",
        "\n",
        "    print(\"Fixed COCO:\", json_path)\n",
        "\n",
        "fix_coco(\"/content/coco_dataset/annotations/instances_train.json\")\n",
        "fix_coco(\"/content/coco_dataset/annotations/instances_val.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDBqv1A6-H8W",
        "outputId": "431df9e4-73c5-414e-8b9b-6ba0c80cb13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed COCO: /content/coco_dataset/annotations/instances_train.json\n",
            "Fixed COCO: /content/coco_dataset/annotations/instances_val.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_faster_rcnn.py\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "import torchvision.transforms as T\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import tempfile\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate(model, dataloader, ann_file, device):\n",
        "    model.eval()\n",
        "    coco_gt = COCO(ann_file)\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in dataloader:\n",
        "            imgs = [img.to(device) for img in imgs]\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            for target, output in zip(targets, outputs):\n",
        "                image_id = target[\"image_id\"].item()\n",
        "\n",
        "                for box, score, label in zip(output[\"boxes\"], output[\"scores\"], output[\"labels\"]):\n",
        "                    x1, y1, x2, y2 = box.tolist()\n",
        "                    results.append({\n",
        "                        \"image_id\": image_id,\n",
        "                        \"category_id\": int(label),\n",
        "                        \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
        "                        \"score\": float(score)\n",
        "                    })\n",
        "\n",
        "    # Save predictions\n",
        "    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n",
        "        json.dump(results, f)\n",
        "        temp_path = f.name\n",
        "\n",
        "    coco_dt = coco_gt.loadRes(temp_path)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    return coco_eval.stats[1]  # mAP50\n",
        "\n",
        "def train():\n",
        "    train_ann = \"/content/coco_dataset/annotations/instances_train.json\"\n",
        "    val_ann = \"/content/coco_dataset/annotations/instances_val.json\"\n",
        "    train_img = \"/content/coco_dataset/images/train\"\n",
        "    val_img = \"/content/coco_dataset/images/val\"\n",
        "\n",
        "    trans = T.Compose([T.ToTensor()])\n",
        "\n",
        "    train_ds = CocoDetection(train_img, train_ann, transforms=trans)\n",
        "    val_ds   = CocoDetection(val_img, val_ann, transforms=trans)\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=4, shuffle=True,\n",
        "                          num_workers=2, collate_fn=collate_fn)\n",
        "    val_dl   = DataLoader(val_ds, batch_size=4, shuffle=False,\n",
        "                          num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "    num_classes = 1 + 8  # background + 8 classes\n",
        "    model = get_model(num_classes).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    best_map50 = 0\n",
        "\n",
        "    for epoch in range(5):\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "\n",
        "        model.train()\n",
        "        for imgs, targets in train_dl:\n",
        "            imgs = [img.to(DEVICE) for img in imgs]\n",
        "            target_list = []\n",
        "            for i, t in enumerate(targets):\n",
        "                boxes = torch.tensor(t[\"boxes\"], dtype=torch.float32).to(DEVICE)\n",
        "                labels = torch.tensor(t[\"labels\"], dtype=torch.int64).to(DEVICE)\n",
        "\n",
        "                target_list.append({\"boxes\": boxes, \"labels\": labels})\n",
        "\n",
        "            loss_dict = model(imgs, target_list)\n",
        "            loss = sum(loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Evaluating...\")\n",
        "        map50 = evaluate(model, val_dl, val_ann, DEVICE)\n",
        "\n",
        "        if map50 > best_map50:\n",
        "            best_map50 = map50\n",
        "            torch.save(model.state_dict(), \"/content/faster_rcnn_best.pth\")\n",
        "            print(\"Saved BEST model!\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    print(\"BEST mAP50:\", best_map50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGVt2cyb-IzY",
        "outputId": "a51b70ee-c866-45f4-bf36-6229eb9917bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_faster_rcnn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_faster_rcnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sfOi2XQ-MBE",
        "outputId": "b294a152-88e9-4e0f-ee09-25433b99e451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Epoch 1\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_faster_rcnn.py\", line 116, in <module>\n",
            "    train()\n",
            "  File \"/content/train_faster_rcnn.py\", line 88, in train\n",
            "    for imgs, targets in train_dl:\n",
            "                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1344, in _next_data\n",
            "    return self._process_data(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1370, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_utils.py\", line 706, in reraise\n",
            "    raise exception\n",
            "TypeError: Caught TypeError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torchvision/datasets/coco.py\", line 57, in __getitem__\n",
            "    image, target = self.transforms(image, target)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: Compose.__call__() takes 2 positional arguments but 3 were given\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform():\n",
        "    return T.Compose([\n",
        "        T.ToTensor()\n",
        "    ])\n",
        "\n",
        "class CocoTransform:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = self.transforms(image)\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "Czi9nraj-P9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_faster_rcnn.py\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import tempfile\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Needed for batching\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Standard image transform\n",
        "def get_transform():\n",
        "    return T.Compose([\n",
        "        T.ToTensor()\n",
        "    ])\n",
        "\n",
        "# Wrapper so transforms accept (image, target)\n",
        "class CocoTransform:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = self.transforms(image)\n",
        "        return image, target\n",
        "\n",
        "# Load Faster-RCNN model\n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# COCO evaluation wrapper\n",
        "def evaluate(model, dataloader, ann_file, device):\n",
        "    model.eval()\n",
        "    coco_gt = COCO(ann_file)\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in dataloader:\n",
        "            imgs = [img.to(device) for img in imgs]\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            for target, output in zip(targets, outputs):\n",
        "                image_id = target[\"image_id\"].item()\n",
        "\n",
        "                for box, score, label in zip(output[\"boxes\"], output[\"scores\"], output[\"labels\"]):\n",
        "                    x1, y1, x2, y2 = box.tolist()\n",
        "                    results.append({\n",
        "                        \"image_id\": image_id,\n",
        "                        \"category_id\": int(label),\n",
        "                        \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
        "                        \"score\": float(score)\n",
        "                    })\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
        "        json.dump(results, f)\n",
        "        temp_json = f.name\n",
        "\n",
        "    coco_dt = coco_gt.loadRes(temp_json)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    return coco_eval.stats[1]  # mAP@50\n",
        "\n",
        "def train():\n",
        "    train_img = \"/content/coco_dataset/images/train\"\n",
        "    val_img = \"/content/coco_dataset/images/val\"\n",
        "    train_ann = \"/content/coco_dataset/annotations/instances_train.json\"\n",
        "    val_ann = \"/content/coco_dataset/annotations/instances_val.json\"\n",
        "\n",
        "    print(\"loading annotations...\")\n",
        "    train_ds = CocoDetection(train_img, train_ann, transforms=CocoTransform(get_transform()))\n",
        "    val_ds = CocoDetection(val_img, val_ann, transforms=CocoTransform(get_transform()))\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "    val_dl = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "    num_classes = 1 + 8  # background + 8 disease classes\n",
        "    model = get_model(num_classes).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    best_map50 = 0.0\n",
        "\n",
        "    for epoch in range(5):\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        model.train()\n",
        "\n",
        "        for imgs, targets in train_dl:\n",
        "            imgs = [img.to(DEVICE) for img in imgs]\n",
        "            formatted = []\n",
        "\n",
        "            for t in targets:\n",
        "                boxes = torch.tensor(t[\"boxes\"], dtype=torch.float32).to(DEVICE)\n",
        "                labels = torch.tensor(t[\"labels\"], dtype=torch.int64).to(DEVICE)\n",
        "                formatted.append({\"boxes\": boxes, \"labels\": labels})\n",
        "\n",
        "            loss_dict = model(imgs, formatted)\n",
        "            loss = sum(loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Evaluating...\")\n",
        "        map50 = evaluate(model, val_dl, val_ann, DEVICE)\n",
        "\n",
        "        if map50 > best_map50:\n",
        "            best_map50 = map50\n",
        "            torch.save(model.state_dict(), \"/content/faster_rcnn_best.pth\")\n",
        "            print(\"Saved BEST model!\")\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "    print(\"Best mAP50:\", best_map50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWOh95OL-dDV",
        "outputId": "416c2ef4-07d7-4567-8d39-0c8972787dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_faster_rcnn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_faster_rcnn.py\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import tempfile, json, numpy as np\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def get_transform():\n",
        "    return T.Compose([T.ToTensor()])\n",
        "\n",
        "class CocoTransform:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = self.transforms(image)\n",
        "        return image, target\n",
        "\n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def evaluate(model, dataloader, ann_file, device):\n",
        "    model.eval()\n",
        "    coco_gt = COCO(ann_file)\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in dataloader:\n",
        "            imgs = [img.to(device) for img in imgs]\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            for t_list, output in zip(targets, outputs):\n",
        "                image_id = t_list[0][\"image_id\"]  # same for all anns in that image\n",
        "                image_id = int(image_id)\n",
        "\n",
        "                for box, score, label in zip(output[\"boxes\"], output[\"scores\"], output[\"labels\"]):\n",
        "                    x1, y1, x2, y2 = box.tolist()\n",
        "                    results.append({\n",
        "                        \"image_id\": image_id,\n",
        "                        \"category_id\": int(label),\n",
        "                        \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
        "                        \"score\": float(score)\n",
        "                    })\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
        "        json.dump(results, f)\n",
        "        pred_file = f.name\n",
        "\n",
        "    coco_dt = coco_gt.loadRes(pred_file)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    return coco_eval.stats[1]  # mAP50\n",
        "\n",
        "def train():\n",
        "    train_img = \"/content/coco_dataset/images/train\"\n",
        "    val_img   = \"/content/coco_dataset/images/val\"\n",
        "    train_ann = \"/content/coco_dataset/annotations/instances_train.json\"\n",
        "    val_ann   = \"/content/coco_dataset/annotations/instances_val.json\"\n",
        "\n",
        "    print(\"Loading datasets...\")\n",
        "\n",
        "    train_ds = CocoDetection(train_img, train_ann, transforms=CocoTransform(get_transform()))\n",
        "    val_ds   = CocoDetection(val_img, val_ann, transforms=CocoTransform(get_transform()))\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=4, shuffle=True,\n",
        "                          num_workers=2, collate_fn=collate_fn)\n",
        "    val_dl   = DataLoader(val_ds, batch_size=4, shuffle=False,\n",
        "                          num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "    num_classes = 1 + 8  # background + 8 disease classes\n",
        "    model = get_model(num_classes).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    best_map50 = 0.0\n",
        "\n",
        "    for epoch in range(5):\n",
        "        print(f\"\\n========== EPOCH {epoch+1} ==========\")\n",
        "        model.train()\n",
        "\n",
        "        for imgs, targets in train_dl:\n",
        "            imgs = [img.to(DEVICE) for img in imgs]\n",
        "            formatted = []\n",
        "\n",
        "            for t_list in targets:\n",
        "                boxes = []\n",
        "                labels = []\n",
        "\n",
        "                for ann in t_list:\n",
        "                    x, y, w, h = ann[\"bbox\"]\n",
        "                    boxes.append([x, y, x+w, y+h])\n",
        "                    labels.append(ann[\"category_id\"])\n",
        "\n",
        "                if len(boxes) == 0:\n",
        "                    boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "                    labels = torch.zeros((0,), dtype=torch.int64)\n",
        "                else:\n",
        "                    boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "                    labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "                formatted.append({\n",
        "                    \"boxes\": boxes.to(DEVICE),\n",
        "                    \"labels\": labels.to(DEVICE)\n",
        "                })\n",
        "\n",
        "            loss_dict = model(imgs, formatted)\n",
        "            loss = sum(loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Evaluating...\")\n",
        "        map50 = evaluate(model, val_dl, val_ann, DEVICE)\n",
        "\n",
        "        if map50 > best_map50:\n",
        "            best_map50 = map50\n",
        "            torch.save(model.state_dict(), \"/content/faster_rcnn_best.pth\")\n",
        "            print(\"🔥 Saved NEW BEST model (mAP50={:.3f})\".format(map50))\n",
        "\n",
        "    print(\"\\nTraining Completed.\")\n",
        "    print(\"Best mAP50:\", best_map50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2idVYj6-vgm",
        "outputId": "6e913727-7427-44ab-fb91-843661e2c8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_faster_rcnn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_faster_rcnn.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPR2QJgh_CEq",
        "outputId": "4dc91468-d040-46bf-9915-afab3540465c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "\n",
            "========== EPOCH 1 ==========\n",
            "Evaluating...\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.07s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.093\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.048\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.088\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.088\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.123\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.126\n",
            "🔥 Saved NEW BEST model (mAP50=0.040)\n",
            "\n",
            "========== EPOCH 2 ==========\n",
            "Evaluating...\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.07s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.050\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.117\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.048\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.060\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.092\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.128\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.168\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.110\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.178\n",
            "🔥 Saved NEW BEST model (mAP50=0.117)\n",
            "\n",
            "========== EPOCH 3 ==========\n",
            "Evaluating...\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.34s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.11s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.129\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.020\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.048\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.052\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.153\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.128\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.280\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.280\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.162\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.244\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.556\n",
            "🔥 Saved NEW BEST model (mAP50=0.129)\n",
            "\n",
            "========== EPOCH 4 ==========\n",
            "Evaluating...\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.09s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.085\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.166\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.074\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.102\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.170\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.289\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.298\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.181\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.611\n",
            "🔥 Saved NEW BEST model (mAP50=0.166)\n",
            "\n",
            "========== EPOCH 5 ==========\n",
            "Evaluating...\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.08s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.093\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.192\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.083\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.096\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.216\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.254\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.254\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.101\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.263\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.615\n",
            "🔥 Saved NEW BEST model (mAP50=0.192)\n",
            "\n",
            "Training Completed.\n",
            "Best mAP50: 0.19208315541349533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mAP@0.50:\n",
        "\n",
        "Epoch 1 → 0.040\n",
        "\n",
        "Epoch 2 → 0.117\n",
        "\n",
        "Epoch 3 → 0.129\n",
        "\n",
        "Epoch 4 → 0.166 ⭐ Best so far!\n",
        "\n",
        "mAP@[0.50:0.95]:\n",
        "\n",
        "Epoch 1 → 0.012\n",
        "\n",
        "Epoch 2 → 0.050\n",
        "\n",
        "Epoch 3 → 0.054\n",
        "\n",
        "Epoch 4 → 0.085 🔥\n",
        "\n",
        "Recall (AR@100):\n",
        "\n",
        "Epoch 1 → 0.088\n",
        "\n",
        "Epoch 2 → 0.168\n",
        "\n",
        "Epoch 3 → 0.280\n",
        "\n",
        "Epoch 4 → 0.298"
      ],
      "metadata": {
        "id": "DLQXaKAECGKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Best mAP@0.50 = 0.1921\n",
        "\n",
        "This is very strong for this dataset.\n",
        "\n",
        "📌 Best mAP@[0.50:0.95] = 0.093"
      ],
      "metadata": {
        "id": "zhX-28V3Clhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/yolo_train/exp_final/weights/best.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "COY3Vg3bBiLk",
        "outputId": "f848371f-c712-4f09-ec29-9ae48ae5322f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0a36d60a-7fdb-477d-ab0a-deab7a7daf01\", \"best.pt\", 6249177)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/faster_rcnn_best.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ua_N4Ed6BeF8",
        "outputId": "92c1c915-065f-47c2-c932-311e23f73a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1e03efb9-7d55-4c80-a639-d00dc98e0037\", \"faster_rcnn_best.pth\", 165873447)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/chestxray_models.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "    zipf.write('/content/yolo_train/exp_final/weights/best.pt', arcname='best_yolo.pt')\n",
        "    zipf.write('/content/faster_rcnn_best.pth', arcname='best_faster_rcnn.pth')\n",
        "\n",
        "print(\"ZIP created:\", zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaYe1W7VFOlT",
        "outputId": "6a1e50d7-b073-4faa-ea57-c88f9ec4dc18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP created: /content/chestxray_models.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/chestxray_models.zip')\n"
      ],
      "metadata": {
        "id": "0E-Pc1oNFdsc",
        "outputId": "70cbc5ba-b29c-41bc-dab0-7e530a9d94e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_93190eea-7b99-48f0-bde9-8580f6f1b571\", \"chestxray_models.zip\", 172122862)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}